2022-08-18 17:10:14 | INFO | fairseq.file_utils | loading archive file pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128
2022-08-18 17:10:15 | INFO | fairseq.tasks.masked_lm | dictionary: 51199 types
2022-08-18 17:10:19 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair1212:17242', 'distributed_port': 17242, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': 50000, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_base', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=True, continue_once=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=17242, distributed_rank=0, distributed_world_size=128, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=True, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, include_target_tokens=False, keep_best_checkpoints=-1, keep_interval_updates=1, keep_interval_updates_pattern=50000, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0006], lr_scheduler='polynomial_decay', mask_impl='fix', mask_multiple_length=1, mask_prob=0.2, mask_prob_range='none', mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_positions=512, max_source_positions=512, max_tokens=None, max_tokens_valid=None, max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, seqlen_masking_boundary='none', seqlen_masking_flag=False, seqlen_masking_granularity='none', shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='masked_lm', tensorboard_logdir='/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='500000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_updates=24000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'masked_lm', 'data': '/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.2, 'mask_prob_range': 'none', 'mask_impl': 'fix', 'seqlen_masking_flag': False, 'seqlen_masking_granularity': 'none', 'seqlen_masking_boundary': 'none', 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': True, 'tpu': False, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
1. Google-RE
roberta_base_mp0.2
{'relation': 'place_of_birth',
 'template': '[X] was born in [Y] .',
 'template_negated': '[X] was not born '
                     'in [Y] .'}
{'dataset_filename': 'data/Google_RE/place_of_birth_test.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was born in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/place_of_birth', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Loading roberta model...
<lama.modules.roberta_connector.Roberta object at 0x7f291ed4ddd0>
2937
2404
distinct template facts: 2404
  0%|          | 0/76 [00:00<?, ?it/s]  1%|▏         | 1/76 [00:10<13:13, 10.58s/it]  3%|▎         | 2/76 [00:11<05:51,  4.75s/it]  4%|▍         | 3/76 [00:11<03:30,  2.89s/it]  5%|▌         | 4/76 [00:12<02:25,  2.03s/it]  7%|▋         | 5/76 [00:13<01:49,  1.55s/it]  8%|▊         | 6/76 [00:14<01:27,  1.25s/it]  9%|▉         | 7/76 [00:14<01:13,  1.06s/it] 11%|█         | 8/76 [00:15<01:05,  1.04it/s] 12%|█▏        | 9/76 [00:16<00:58,  1.14it/s] 13%|█▎        | 10/76 [00:16<00:54,  1.22it/s] 14%|█▍        | 11/76 [00:17<00:51,  1.26it/s] 16%|█▌        | 12/76 [00:18<00:48,  1.31it/s] 17%|█▋        | 13/76 [00:18<00:46,  1.36it/s] 18%|█▊        | 14/76 [00:19<00:47,  1.29it/s] 20%|█▉        | 15/76 [00:20<00:45,  1.34it/s] 21%|██        | 16/76 [00:21<00:43,  1.38it/s] 22%|██▏       | 17/76 [00:21<00:43,  1.34it/s] 24%|██▎       | 18/76 [00:22<00:42,  1.36it/s] 25%|██▌       | 19/76 [00:23<00:40,  1.40it/s] 26%|██▋       | 20/76 [00:23<00:39,  1.41it/s] 28%|██▊       | 21/76 [00:24<00:38,  1.43it/s] 29%|██▉       | 22/76 [00:25<00:37,  1.42it/s] 30%|███       | 23/76 [00:26<00:36,  1.45it/s] 32%|███▏      | 24/76 [00:26<00:35,  1.46it/s] 33%|███▎      | 25/76 [00:27<00:35,  1.45it/s] 34%|███▍      | 26/76 [00:28<00:34,  1.45it/s] 36%|███▌      | 27/76 [00:28<00:33,  1.47it/s] 37%|███▋      | 28/76 [00:29<00:32,  1.48it/s] 38%|███▊      | 29/76 [00:30<00:31,  1.48it/s] 39%|███▉      | 30/76 [00:30<00:30,  1.49it/s] 41%|████      | 31/76 [00:31<00:30,  1.46it/s] 42%|████▏     | 32/76 [00:32<00:30,  1.46it/s] 43%|████▎     | 33/76 [00:32<00:29,  1.46it/s] 45%|████▍     | 34/76 [00:33<00:29,  1.42it/s] 46%|████▌     | 35/76 [00:34<00:28,  1.42it/s] 47%|████▋     | 36/76 [00:34<00:27,  1.44it/s] 49%|████▊     | 37/76 [00:35<00:27,  1.44it/s] 50%|█████     | 38/76 [00:36<00:26,  1.46it/s] 51%|█████▏    | 39/76 [00:36<00:25,  1.48it/s] 53%|█████▎    | 40/76 [00:37<00:24,  1.46it/s] 54%|█████▍    | 41/76 [00:38<00:23,  1.47it/s] 55%|█████▌    | 42/76 [00:39<00:23,  1.47it/s] 57%|█████▋    | 43/76 [00:39<00:22,  1.48it/s] 58%|█████▊    | 44/76 [00:40<00:21,  1.49it/s] 59%|█████▉    | 45/76 [00:41<00:20,  1.50it/s] 61%|██████    | 46/76 [00:41<00:20,  1.50it/s] 62%|██████▏   | 47/76 [00:42<00:19,  1.46it/s] 63%|██████▎   | 48/76 [00:43<00:19,  1.47it/s] 64%|██████▍   | 49/76 [00:43<00:18,  1.47it/s] 66%|██████▌   | 50/76 [00:44<00:17,  1.48it/s] 67%|██████▋   | 51/76 [00:45<00:17,  1.46it/s] 68%|██████▊   | 52/76 [00:45<00:16,  1.46it/s] 70%|██████▉   | 53/76 [00:46<00:15,  1.47it/s] 71%|███████   | 54/76 [00:47<00:15,  1.45it/s] 72%|███████▏  | 55/76 [00:47<00:14,  1.47it/s] 74%|███████▎  | 56/76 [00:48<00:13,  1.48it/s] 75%|███████▌  | 57/76 [00:49<00:12,  1.49it/s] 76%|███████▋  | 58/76 [00:49<00:12,  1.47it/s] 78%|███████▊  | 59/76 [00:50<00:11,  1.48it/s] 79%|███████▉  | 60/76 [00:51<00:10,  1.49it/s] 80%|████████  | 61/76 [00:51<00:10,  1.49it/s] 82%|████████▏ | 62/76 [00:52<00:09,  1.49it/s] 83%|████████▎ | 63/76 [00:53<00:08,  1.49it/s] 84%|████████▍ | 64/76 [00:53<00:08,  1.48it/s] 86%|████████▌ | 65/76 [00:54<00:07,  1.46it/s] 87%|████████▋ | 66/76 [00:55<00:06,  1.45it/s] 88%|████████▊ | 67/76 [00:56<00:06,  1.41it/s] 89%|████████▉ | 68/76 [00:56<00:05,  1.37it/s] 91%|█████████ | 69/76 [00:57<00:05,  1.37it/s] 92%|█████████▏| 70/76 [00:58<00:04,  1.38it/s] 93%|█████████▎| 71/76 [00:59<00:03,  1.36it/s] 95%|█████████▍| 72/76 [00:59<00:02,  1.39it/s] 96%|█████████▌| 73/76 [01:01<00:02,  1.02it/s] 97%|█████████▋| 74/76 [01:02<00:01,  1.12it/s] 99%|█████████▊| 75/76 [01:02<00:00,  1.16it/s]100%|██████████| 76/76 [01:02<00:00,  1.57it/s]100%|██████████| 76/76 [01:02<00:00,  1.21it/s]Moving model to CUDA

all_samples: 2404
list_of_results: 2404
global MRR: 0.19197251380178598
global Precision at 10: 0.3211314475873544
global Precision at 1: 0.12396006655574043


P@1 : 0.12396006655574043

{'relation': 'date_of_birth',
 'template': '[X] (born [Y]).',
 'template_negated': '[X] (not born '
                     '[Y]).'}
{'dataset_filename': 'data/Google_RE/date_of_birth_test.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] (born [Y]).', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/date_of_birth', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ed4ddd0>
1825
1565
distinct template facts: 1565
  0%|          | 0/49 [00:00<?, ?it/s]  2%|▏         | 1/49 [00:00<00:31,  1.53it/s]  4%|▍         | 2/49 [00:01<00:31,  1.49it/s]  6%|▌         | 3/49 [00:02<00:30,  1.49it/s]  8%|▊         | 4/49 [00:02<00:29,  1.51it/s] 10%|█         | 5/49 [00:03<00:29,  1.51it/s] 12%|█▏        | 6/49 [00:04<00:28,  1.48it/s] 14%|█▍        | 7/49 [00:04<00:28,  1.49it/s] 16%|█▋        | 8/49 [00:05<00:27,  1.48it/s] 18%|█▊        | 9/49 [00:06<00:27,  1.47it/s] 20%|██        | 10/49 [00:06<00:26,  1.49it/s] 22%|██▏       | 11/49 [00:07<00:25,  1.47it/s] 24%|██▍       | 12/49 [00:08<00:24,  1.50it/s] 27%|██▋       | 13/49 [00:08<00:23,  1.51it/s] 29%|██▊       | 14/49 [00:09<00:23,  1.52it/s] 31%|███       | 15/49 [00:10<00:22,  1.51it/s] 33%|███▎      | 16/49 [00:10<00:21,  1.52it/s] 35%|███▍      | 17/49 [00:11<00:21,  1.51it/s] 37%|███▋      | 18/49 [00:12<00:20,  1.49it/s] 39%|███▉      | 19/49 [00:12<00:20,  1.50it/s] 41%|████      | 20/49 [00:13<00:21,  1.35it/s] 43%|████▎     | 21/49 [00:14<00:20,  1.39it/s] 45%|████▍     | 22/49 [00:14<00:18,  1.43it/s] 47%|████▋     | 23/49 [00:15<00:18,  1.44it/s] 49%|████▉     | 24/49 [00:16<00:17,  1.46it/s] 51%|█████     | 25/49 [00:16<00:16,  1.46it/s] 53%|█████▎    | 26/49 [00:17<00:15,  1.45it/s] 55%|█████▌    | 27/49 [00:18<00:15,  1.44it/s] 57%|█████▋    | 28/49 [00:19<00:14,  1.43it/s] 59%|█████▉    | 29/49 [00:19<00:13,  1.45it/s] 61%|██████    | 30/49 [00:20<00:12,  1.47it/s] 63%|██████▎   | 31/49 [00:21<00:12,  1.46it/s] 65%|██████▌   | 32/49 [00:21<00:11,  1.47it/s] 67%|██████▋   | 33/49 [00:22<00:10,  1.47it/s] 69%|██████▉   | 34/49 [00:23<00:10,  1.45it/s] 71%|███████▏  | 35/49 [00:23<00:09,  1.45it/s] 73%|███████▎  | 36/49 [00:24<00:08,  1.46it/s] 76%|███████▌  | 37/49 [00:25<00:08,  1.49it/s] 78%|███████▊  | 38/49 [00:25<00:07,  1.49it/s] 80%|███████▉  | 39/49 [00:26<00:06,  1.46it/s] 82%|████████▏ | 40/49 [00:27<00:06,  1.45it/s] 84%|████████▎ | 41/49 [00:27<00:05,  1.42it/s] 86%|████████▌ | 42/49 [00:28<00:04,  1.42it/s] 88%|████████▊ | 43/49 [00:29<00:04,  1.35it/s] 90%|████████▉ | 44/49 [00:30<00:03,  1.36it/s] 92%|█████████▏| 45/49 [00:31<00:03,  1.32it/s] 94%|█████████▍| 46/49 [00:31<00:02,  1.35it/s] 96%|█████████▌| 47/49 [00:32<00:01,  1.36it/s] 98%|█████████▊| 48/49 [00:33<00:00,  1.38it/s]100%|██████████| 49/49 [00:33<00:00,  1.40it/s]100%|██████████| 49/49 [00:33<00:00,  1.45it/s]
all_samples: 1565
list_of_results: 1565
global MRR: 0.07828284175905323
global Precision at 10: 0.1718849840255591
global Precision at 1: 0.020447284345047924


P@1 : 0.020447284345047924

{'relation': 'place_of_death',
 'template': '[X] died in [Y] .',
 'template_negated': '[X] did not die '
                     'in [Y] .'}
{'dataset_filename': 'data/Google_RE/place_of_death_test.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] died in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/place_of_death', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ed4ddd0>
766
649
distinct template facts: 649
  0%|          | 0/21 [00:00<?, ?it/s]  5%|▍         | 1/21 [00:00<00:13,  1.51it/s] 10%|▉         | 2/21 [00:01<00:12,  1.50it/s] 14%|█▍        | 3/21 [00:02<00:12,  1.48it/s] 19%|█▉        | 4/21 [00:02<00:11,  1.50it/s] 24%|██▍       | 5/21 [00:03<00:10,  1.49it/s] 29%|██▊       | 6/21 [00:04<00:09,  1.50it/s] 33%|███▎      | 7/21 [00:04<00:09,  1.42it/s] 38%|███▊      | 8/21 [00:05<00:09,  1.43it/s] 43%|████▎     | 9/21 [00:06<00:08,  1.45it/s] 48%|████▊     | 10/21 [00:06<00:07,  1.43it/s] 52%|█████▏    | 11/21 [00:07<00:06,  1.46it/s] 57%|█████▋    | 12/21 [00:08<00:06,  1.45it/s] 62%|██████▏   | 13/21 [00:08<00:05,  1.45it/s] 67%|██████▋   | 14/21 [00:09<00:04,  1.45it/s] 71%|███████▏  | 15/21 [00:10<00:04,  1.46it/s] 76%|███████▌  | 16/21 [00:10<00:03,  1.47it/s] 81%|████████  | 17/21 [00:11<00:02,  1.47it/s] 86%|████████▌ | 18/21 [00:12<00:02,  1.48it/s] 90%|█████████ | 19/21 [00:12<00:01,  1.48it/s] 95%|█████████▌| 20/21 [00:13<00:00,  1.44it/s]100%|██████████| 21/21 [00:13<00:00,  1.84it/s]100%|██████████| 21/21 [00:13<00:00,  1.51it/s]
all_samples: 649
list_of_results: 649
global MRR: 0.009698972438413937
global Precision at 10: 0.0030816640986132513
global Precision at 1: 0.0030816640986132513


P@1 : 0.0030816640986132513

2022-08-18 17:13:13 | INFO | fairseq.file_utils | loading archive file pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128
2022-08-18 17:13:15 | INFO | fairseq.tasks.masked_lm | dictionary: 51199 types
2022-08-18 17:13:18 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair1212:17242', 'distributed_port': 17242, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': 50000, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_base', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=True, continue_once=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=17242, distributed_rank=0, distributed_world_size=128, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=True, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, include_target_tokens=False, keep_best_checkpoints=-1, keep_interval_updates=1, keep_interval_updates_pattern=50000, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0006], lr_scheduler='polynomial_decay', mask_impl='fix', mask_multiple_length=1, mask_prob=0.2, mask_prob_range='none', mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_positions=512, max_source_positions=512, max_tokens=None, max_tokens_valid=None, max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, seqlen_masking_boundary='none', seqlen_masking_flag=False, seqlen_masking_granularity='none', shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='masked_lm', tensorboard_logdir='/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='500000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_updates=24000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'masked_lm', 'data': '/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.2, 'mask_prob_range': 'none', 'mask_impl': 'fix', 'seqlen_masking_flag': False, 'seqlen_masking_granularity': 'none', 'seqlen_masking_boundary': 'none', 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': True, 'tpu': False, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
@@@ roberta_base_mp0.2 - mean P@1: 0.04916300499980054
2. T-REx
roberta_base_mp0.2
{'description': 'most specific known '
                '(e.g. city instead of '
                'country, or hospital '
                'instead of city) birth '
                'location of a person, '
                'animal or fictional '
                'character',
 'label': 'place of birth',
 'relation': 'P19',
 'template': '[X] was born in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P19.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was born in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P19', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Loading roberta model...
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
944
780
distinct template facts: 780
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:20,  1.20it/s]  8%|▊         | 2/25 [00:01<00:16,  1.37it/s] 12%|█▏        | 3/25 [00:02<00:15,  1.46it/s] 16%|█▌        | 4/25 [00:02<00:13,  1.52it/s] 20%|██        | 5/25 [00:03<00:12,  1.57it/s] 24%|██▍       | 6/25 [00:03<00:11,  1.59it/s] 28%|██▊       | 7/25 [00:04<00:11,  1.60it/s] 32%|███▏      | 8/25 [00:05<00:10,  1.62it/s] 36%|███▌      | 9/25 [00:05<00:09,  1.64it/s] 40%|████      | 10/25 [00:06<00:09,  1.64it/s] 44%|████▍     | 11/25 [00:06<00:08,  1.64it/s] 48%|████▊     | 12/25 [00:07<00:07,  1.64it/s] 52%|█████▏    | 13/25 [00:08<00:07,  1.64it/s] 56%|█████▌    | 14/25 [00:08<00:06,  1.63it/s] 60%|██████    | 15/25 [00:09<00:06,  1.64it/s] 64%|██████▍   | 16/25 [00:10<00:05,  1.65it/s] 68%|██████▊   | 17/25 [00:10<00:04,  1.65it/s] 72%|███████▏  | 18/25 [00:11<00:04,  1.66it/s] 76%|███████▌  | 19/25 [00:11<00:03,  1.65it/s] 80%|████████  | 20/25 [00:12<00:03,  1.46it/s] 84%|████████▍ | 21/25 [00:13<00:02,  1.49it/s] 88%|████████▊ | 22/25 [00:13<00:01,  1.52it/s] 92%|█████████▏| 23/25 [00:14<00:01,  1.55it/s] 96%|█████████▌| 24/25 [00:15<00:00,  1.57it/s]100%|██████████| 25/25 [00:15<00:00,  1.93it/s]100%|██████████| 25/25 [00:15<00:00,  1.62it/s]Moving model to CUDA

all_samples: 780
list_of_results: 780
global MRR: 0.21240310777478158
global Precision at 10: 0.33589743589743587
global Precision at 1: 0.15


P@1 : 0.15

{'description': 'most specific known '
                '(e.g. city instead of '
                'country, or hospital '
                'instead of city) death '
                'location of a person, '
                'animal or fictional '
                'character',
 'label': 'place of death',
 'relation': 'P20',
 'template': '[X] died in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P20.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] died in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P20', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
953
817
distinct template facts: 817
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:00<00:15,  1.66it/s]  8%|▊         | 2/26 [00:01<00:14,  1.67it/s] 12%|█▏        | 3/26 [00:01<00:14,  1.64it/s] 15%|█▌        | 4/26 [00:02<00:13,  1.64it/s] 19%|█▉        | 5/26 [00:03<00:12,  1.63it/s] 23%|██▎       | 6/26 [00:03<00:12,  1.62it/s] 27%|██▋       | 7/26 [00:04<00:11,  1.63it/s] 31%|███       | 8/26 [00:04<00:10,  1.64it/s] 35%|███▍      | 9/26 [00:05<00:10,  1.64it/s] 38%|███▊      | 10/26 [00:06<00:09,  1.65it/s] 42%|████▏     | 11/26 [00:06<00:09,  1.64it/s] 46%|████▌     | 12/26 [00:07<00:08,  1.63it/s] 50%|█████     | 13/26 [00:07<00:07,  1.64it/s] 54%|█████▍    | 14/26 [00:08<00:07,  1.64it/s] 58%|█████▊    | 15/26 [00:09<00:06,  1.65it/s] 62%|██████▏   | 16/26 [00:09<00:06,  1.64it/s] 65%|██████▌   | 17/26 [00:10<00:05,  1.61it/s] 69%|██████▉   | 18/26 [00:11<00:04,  1.62it/s] 73%|███████▎  | 19/26 [00:11<00:04,  1.64it/s] 77%|███████▋  | 20/26 [00:12<00:03,  1.63it/s] 81%|████████  | 21/26 [00:12<00:03,  1.64it/s] 85%|████████▍ | 22/26 [00:13<00:02,  1.64it/s] 88%|████████▊ | 23/26 [00:14<00:01,  1.64it/s] 92%|█████████▏| 24/26 [00:14<00:01,  1.63it/s] 96%|█████████▌| 25/26 [00:15<00:00,  1.63it/s]100%|██████████| 26/26 [00:15<00:00,  1.89it/s]100%|██████████| 26/26 [00:15<00:00,  1.66it/s]
all_samples: 817
list_of_results: 817
global MRR: 0.025188289885568073
global Precision at 10: 0.022031823745410038
global Precision at 1: 0.011015911872705019


P@1 : 0.011015911872705019

{'description': 'all instances of these '
                'items are instances of '
                'those items; this item '
                'is a class (subset) of '
                'that item. Not to be '
                'confused with P31 '
                '(instance of)',
 'label': 'subclass of',
 'relation': 'P279',
 'template': '[X] is a subclass of [Y] '
             '.',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P279.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a subclass of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P279', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
964
903
distinct template facts: 903
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:19,  1.47it/s]  7%|▋         | 2/29 [00:01<00:17,  1.54it/s] 10%|█         | 3/29 [00:01<00:16,  1.58it/s] 14%|█▍        | 4/29 [00:02<00:15,  1.58it/s] 17%|█▋        | 5/29 [00:03<00:15,  1.57it/s] 21%|██        | 6/29 [00:03<00:14,  1.59it/s] 24%|██▍       | 7/29 [00:04<00:13,  1.60it/s] 28%|██▊       | 8/29 [00:05<00:13,  1.60it/s] 31%|███       | 9/29 [00:05<00:12,  1.60it/s] 34%|███▍      | 10/29 [00:06<00:11,  1.61it/s] 38%|███▊      | 11/29 [00:06<00:11,  1.61it/s] 41%|████▏     | 12/29 [00:07<00:10,  1.61it/s] 45%|████▍     | 13/29 [00:08<00:09,  1.60it/s] 48%|████▊     | 14/29 [00:08<00:09,  1.58it/s] 52%|█████▏    | 15/29 [00:09<00:08,  1.58it/s] 55%|█████▌    | 16/29 [00:10<00:08,  1.59it/s] 59%|█████▊    | 17/29 [00:10<00:07,  1.59it/s] 62%|██████▏   | 18/29 [00:11<00:07,  1.54it/s] 66%|██████▌   | 19/29 [00:12<00:06,  1.56it/s] 69%|██████▉   | 20/29 [00:12<00:05,  1.57it/s] 72%|███████▏  | 21/29 [00:13<00:05,  1.58it/s] 76%|███████▌  | 22/29 [00:13<00:04,  1.58it/s] 79%|███████▉  | 23/29 [00:14<00:03,  1.59it/s] 83%|████████▎ | 24/29 [00:15<00:03,  1.58it/s] 86%|████████▌ | 25/29 [00:15<00:02,  1.58it/s] 90%|████████▉ | 26/29 [00:16<00:01,  1.56it/s] 93%|█████████▎| 27/29 [00:17<00:01,  1.54it/s] 97%|█████████▋| 28/29 [00:17<00:00,  1.52it/s]100%|██████████| 29/29 [00:17<00:00,  1.97it/s]100%|██████████| 29/29 [00:17<00:00,  1.61it/s]
all_samples: 903
list_of_results: 903
global MRR: 0.46480789045397225
global Precision at 10: 0.5913621262458472
global Precision at 1: 0.3920265780730897


P@1 : 0.3920265780730897

{'description': 'language designated as '
                'official by this item',
 'label': 'official language',
 'relation': 'P37',
 'template': 'The official language of '
             '[X] is [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P37.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': 'The official language of [X] is [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P37', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
966
900
distinct template facts: 900
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:16,  1.65it/s]  7%|▋         | 2/29 [00:01<00:16,  1.64it/s] 10%|█         | 3/29 [00:01<00:16,  1.61it/s] 14%|█▍        | 4/29 [00:02<00:15,  1.61it/s] 17%|█▋        | 5/29 [00:03<00:14,  1.61it/s] 21%|██        | 6/29 [00:03<00:14,  1.62it/s] 24%|██▍       | 7/29 [00:04<00:14,  1.52it/s] 28%|██▊       | 8/29 [00:05<00:13,  1.56it/s] 31%|███       | 9/29 [00:05<00:12,  1.59it/s] 34%|███▍      | 10/29 [00:06<00:11,  1.61it/s] 38%|███▊      | 11/29 [00:06<00:11,  1.63it/s] 41%|████▏     | 12/29 [00:07<00:10,  1.64it/s] 45%|████▍     | 13/29 [00:08<00:09,  1.64it/s] 48%|████▊     | 14/29 [00:08<00:09,  1.60it/s] 52%|█████▏    | 15/29 [00:09<00:08,  1.62it/s] 55%|█████▌    | 16/29 [00:09<00:08,  1.62it/s] 59%|█████▊    | 17/29 [00:10<00:07,  1.63it/s] 62%|██████▏   | 18/29 [00:11<00:06,  1.63it/s] 66%|██████▌   | 19/29 [00:11<00:06,  1.63it/s] 69%|██████▉   | 20/29 [00:12<00:05,  1.63it/s] 72%|███████▏  | 21/29 [00:13<00:04,  1.63it/s] 76%|███████▌  | 22/29 [00:13<00:04,  1.63it/s] 79%|███████▉  | 23/29 [00:14<00:03,  1.63it/s] 83%|████████▎ | 24/29 [00:14<00:03,  1.62it/s] 86%|████████▌ | 25/29 [00:15<00:02,  1.62it/s] 90%|████████▉ | 26/29 [00:16<00:01,  1.62it/s] 93%|█████████▎| 27/29 [00:16<00:01,  1.59it/s] 97%|█████████▋| 28/29 [00:17<00:00,  1.59it/s]100%|██████████| 29/29 [00:17<00:00,  1.66it/s]
all_samples: 900
list_of_results: 900
global MRR: 0.6002145633244416
global Precision at 10: 0.8777777777777778
global Precision at 1: 0.45444444444444443


P@1 : 0.45444444444444443

{'description': 'position or specialism '
                'of a player on a team, '
                'e.g. Small Forward',
 'label': 'position played on team / '
          'speciality',
 'relation': 'P413',
 'template': '[X] plays in [Y] position '
             '.',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P413.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] plays in [Y] position .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P413', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
952
952
distinct template facts: 952
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:17,  1.62it/s]  7%|▋         | 2/30 [00:01<00:17,  1.60it/s] 10%|█         | 3/30 [00:01<00:16,  1.59it/s] 13%|█▎        | 4/30 [00:02<00:16,  1.62it/s] 17%|█▋        | 5/30 [00:03<00:15,  1.64it/s] 20%|██        | 6/30 [00:03<00:14,  1.65it/s] 23%|██▎       | 7/30 [00:04<00:13,  1.66it/s] 27%|██▋       | 8/30 [00:04<00:13,  1.66it/s] 30%|███       | 9/30 [00:05<00:12,  1.65it/s] 33%|███▎      | 10/30 [00:06<00:12,  1.62it/s] 37%|███▋      | 11/30 [00:06<00:11,  1.61it/s] 40%|████      | 12/30 [00:07<00:11,  1.62it/s] 43%|████▎     | 13/30 [00:08<00:11,  1.51it/s] 47%|████▋     | 14/30 [00:08<00:10,  1.56it/s] 50%|█████     | 15/30 [00:09<00:09,  1.61it/s] 53%|█████▎    | 16/30 [00:09<00:08,  1.64it/s] 57%|█████▋    | 17/30 [00:10<00:07,  1.66it/s] 60%|██████    | 18/30 [00:11<00:07,  1.68it/s] 63%|██████▎   | 19/30 [00:11<00:06,  1.69it/s] 67%|██████▋   | 20/30 [00:12<00:05,  1.68it/s] 70%|███████   | 21/30 [00:12<00:05,  1.69it/s] 73%|███████▎  | 22/30 [00:13<00:04,  1.69it/s] 77%|███████▋  | 23/30 [00:14<00:04,  1.68it/s] 80%|████████  | 24/30 [00:14<00:03,  1.68it/s] 83%|████████▎ | 25/30 [00:15<00:02,  1.71it/s] 87%|████████▋ | 26/30 [00:15<00:02,  1.74it/s] 90%|█████████ | 27/30 [00:16<00:01,  1.77it/s] 93%|█████████▎| 28/30 [00:16<00:01,  1.78it/s] 97%|█████████▋| 29/30 [00:17<00:00,  1.78it/s]100%|██████████| 30/30 [00:17<00:00,  1.93it/s]100%|██████████| 30/30 [00:17<00:00,  1.69it/s]
all_samples: 952
list_of_results: 952
global MRR: 0.026013424872785428
global Precision at 10: 0.01365546218487395
global Precision at 1: 0.0


P@1 : 0.0

{'description': 'award or recognition '
                'received by a person, '
                'organisation or '
                'creative work',
 'label': 'award received',
 'relation': 'P166',
 'template': '[X] was awarded the [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P166.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was awarded the [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P166', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Relation P166 excluded.
Exception: [Errno 2] No such file or directory: 'data/TREx/P166.jsonl'
{'description': 'network(s) the radio '
                'or television show was '
                'originally aired on,  '
                'including',
 'label': 'original network',
 'relation': 'P449',
 'template': '[X] was originally aired '
             'on [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P449.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was originally aired on [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P449', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
881
808
distinct template facts: 807
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:00<00:13,  1.82it/s]  8%|▊         | 2/26 [00:01<00:13,  1.84it/s] 12%|█▏        | 3/26 [00:01<00:12,  1.84it/s] 15%|█▌        | 4/26 [00:02<00:11,  1.84it/s] 19%|█▉        | 5/26 [00:02<00:11,  1.83it/s] 23%|██▎       | 6/26 [00:03<00:10,  1.84it/s] 27%|██▋       | 7/26 [00:03<00:10,  1.84it/s] 31%|███       | 8/26 [00:04<00:09,  1.82it/s] 35%|███▍      | 9/26 [00:04<00:09,  1.81it/s] 38%|███▊      | 10/26 [00:05<00:08,  1.82it/s] 42%|████▏     | 11/26 [00:06<00:08,  1.83it/s] 46%|████▌     | 12/26 [00:06<00:07,  1.76it/s] 50%|█████     | 13/26 [00:07<00:07,  1.73it/s] 54%|█████▍    | 14/26 [00:07<00:06,  1.72it/s] 58%|█████▊    | 15/26 [00:08<00:06,  1.71it/s] 62%|██████▏   | 16/26 [00:08<00:05,  1.71it/s] 65%|██████▌   | 17/26 [00:09<00:05,  1.70it/s] 69%|██████▉   | 18/26 [00:10<00:05,  1.56it/s] 73%|███████▎  | 19/26 [00:10<00:04,  1.61it/s] 77%|███████▋  | 20/26 [00:11<00:03,  1.66it/s] 81%|████████  | 21/26 [00:12<00:02,  1.68it/s] 85%|████████▍ | 22/26 [00:12<00:02,  1.70it/s] 88%|████████▊ | 23/26 [00:13<00:01,  1.67it/s] 92%|█████████▏| 24/26 [00:13<00:01,  1.68it/s] 96%|█████████▌| 25/26 [00:14<00:00,  1.69it/s]100%|██████████| 26/26 [00:14<00:00,  2.20it/s]100%|██████████| 26/26 [00:14<00:00,  1.78it/s]
all_samples: 807
list_of_results: 807
global MRR: 0.3173194873142761
global Precision at 10: 0.828996282527881
global Precision at 1: 0.14745972738537794


P@1 : 0.14745972738537794

{'description': 'educational '
                'institution attended '
                'by subject',
 'label': 'educated at',
 'relation': 'P69',
 'template': '[X] was educated at the '
             'University of [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P69.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was educated at the University of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P69', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Relation P69 excluded.
Exception: [Errno 2] No such file or directory: 'data/TREx/P69.jsonl'
{'description': 'countries or '
                'administrative '
                'subdivisions, of equal '
                'level, that this item '
                'borders, either by '
                'land or water',
 'label': 'shares border with',
 'relation': 'P47',
 'template': '[X] shares border with '
             '[Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P47.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] shares border with [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P47', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
922
652
distinct template facts: 651
  0%|          | 0/21 [00:00<?, ?it/s]  5%|▍         | 1/21 [00:00<00:11,  1.81it/s] 10%|▉         | 2/21 [00:01<00:10,  1.84it/s] 14%|█▍        | 3/21 [00:01<00:09,  1.88it/s] 19%|█▉        | 4/21 [00:02<00:09,  1.87it/s] 24%|██▍       | 5/21 [00:02<00:08,  1.87it/s] 29%|██▊       | 6/21 [00:03<00:08,  1.86it/s] 33%|███▎      | 7/21 [00:03<00:07,  1.86it/s] 38%|███▊      | 8/21 [00:04<00:07,  1.80it/s] 43%|████▎     | 9/21 [00:04<00:06,  1.73it/s] 48%|████▊     | 10/21 [00:05<00:06,  1.71it/s] 52%|█████▏    | 11/21 [00:06<00:05,  1.74it/s] 57%|█████▋    | 12/21 [00:06<00:05,  1.76it/s] 62%|██████▏   | 13/21 [00:07<00:04,  1.78it/s] 67%|██████▋   | 14/21 [00:07<00:03,  1.81it/s] 71%|███████▏  | 15/21 [00:08<00:03,  1.82it/s] 76%|███████▌  | 16/21 [00:08<00:02,  1.83it/s] 81%|████████  | 17/21 [00:09<00:02,  1.83it/s] 86%|████████▌ | 18/21 [00:09<00:01,  1.83it/s] 90%|█████████ | 19/21 [00:10<00:01,  1.82it/s] 95%|█████████▌| 20/21 [00:11<00:00,  1.83it/s]100%|██████████| 21/21 [00:11<00:00,  2.28it/s]100%|██████████| 21/21 [00:11<00:00,  1.87it/s]
all_samples: 651
list_of_results: 651
global MRR: 0.229987348418653
global Precision at 10: 0.4915514592933948
global Precision at 1: 0.11213517665130568


P@1 : 0.11213517665130568

{'description': 'entity or event that '
                "inspired the subject's "
                'name, or namesake (in '
                'at least one language)',
 'label': 'named after',
 'relation': 'P138',
 'template': '[X] is named after [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P138.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is named after [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P138', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
645
468
distinct template facts: 466
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:00<00:08,  1.60it/s] 13%|█▎        | 2/15 [00:01<00:07,  1.70it/s] 20%|██        | 3/15 [00:01<00:06,  1.77it/s] 27%|██▋       | 4/15 [00:02<00:06,  1.80it/s] 33%|███▎      | 5/15 [00:02<00:05,  1.83it/s] 40%|████      | 6/15 [00:03<00:04,  1.80it/s] 47%|████▋     | 7/15 [00:03<00:04,  1.82it/s] 53%|█████▎    | 8/15 [00:04<00:03,  1.83it/s] 60%|██████    | 9/15 [00:04<00:03,  1.84it/s] 67%|██████▋   | 10/15 [00:05<00:02,  1.85it/s] 73%|███████▎  | 11/15 [00:06<00:02,  1.85it/s] 80%|████████  | 12/15 [00:06<00:01,  1.85it/s] 87%|████████▋ | 13/15 [00:07<00:01,  1.84it/s] 93%|█████████▎| 14/15 [00:07<00:00,  1.83it/s]100%|██████████| 15/15 [00:08<00:00,  2.01it/s]100%|██████████| 15/15 [00:08<00:00,  1.86it/s]
all_samples: 466
list_of_results: 466
global MRR: 0.5060707187249461
global Precision at 10: 0.6630901287553648
global Precision at 1: 0.4227467811158798


P@1 : 0.4227467811158798

{'description': 'language in which a '
                'film or a performance '
                'work was originally '
                'created. Deprecated '
                'for written works; use '
                'P407 ("language of '
                'work or name") '
                'instead.',
 'label': 'original language of film or '
          'TV show',
 'relation': 'P364',
 'template': 'The original language of '
             '[X] is [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P364.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': 'The original language of [X] is [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P364', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
856
756
distinct template facts: 756
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:00<00:12,  1.79it/s]  8%|▊         | 2/24 [00:01<00:12,  1.83it/s] 12%|█▎        | 3/24 [00:01<00:11,  1.82it/s] 17%|█▋        | 4/24 [00:02<00:11,  1.68it/s] 21%|██        | 5/24 [00:02<00:11,  1.71it/s] 25%|██▌       | 6/24 [00:03<00:10,  1.71it/s] 29%|██▉       | 7/24 [00:04<00:09,  1.72it/s] 33%|███▎      | 8/24 [00:04<00:09,  1.75it/s] 38%|███▊      | 9/24 [00:05<00:08,  1.77it/s] 42%|████▏     | 10/24 [00:05<00:07,  1.79it/s] 46%|████▌     | 11/24 [00:06<00:07,  1.79it/s] 50%|█████     | 12/24 [00:06<00:06,  1.76it/s] 54%|█████▍    | 13/24 [00:07<00:06,  1.73it/s] 58%|█████▊    | 14/24 [00:07<00:05,  1.75it/s] 62%|██████▎   | 15/24 [00:08<00:05,  1.78it/s] 67%|██████▋   | 16/24 [00:09<00:04,  1.77it/s] 71%|███████   | 17/24 [00:09<00:03,  1.78it/s] 75%|███████▌  | 18/24 [00:10<00:03,  1.73it/s] 79%|███████▉  | 19/24 [00:10<00:02,  1.74it/s] 83%|████████▎ | 20/24 [00:11<00:02,  1.73it/s] 88%|████████▊ | 21/24 [00:11<00:01,  1.74it/s] 92%|█████████▏| 22/24 [00:12<00:01,  1.53it/s] 96%|█████████▌| 23/24 [00:13<00:00,  1.60it/s]100%|██████████| 24/24 [00:13<00:00,  1.83it/s]100%|██████████| 24/24 [00:13<00:00,  1.74it/s]
all_samples: 756
list_of_results: 756
global MRR: 0.5915609234833706
global Precision at 10: 0.798941798941799
global Precision at 1: 0.47354497354497355


P@1 : 0.47354497354497355

{'description': 'sports teams or clubs '
                'that the subject '
                'currently represents '
                'or formerly '
                'represented',
 'label': 'member of sports team',
 'relation': 'P54',
 'template': '[X] plays with [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P54.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] plays with [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P54', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Relation P54 excluded.
Exception: [Errno 2] No such file or directory: 'data/TREx/P54.jsonl'
{'description': 'organization or club '
                'to which the subject '
                'belongs. Do not use '
                'for membership in '
                'ethnic or social '
                'groups, nor for '
                'holding a position '
                'such as a member of '
                'parliament (use P39 '
                'for that).',
 'label': 'member of',
 'relation': 'P463',
 'template': '[X] is a member of [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P463.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a member of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P463', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
225
203
distinct template facts: 203
  0%|          | 0/7 [00:00<?, ?it/s] 14%|█▍        | 1/7 [00:00<00:03,  1.76it/s] 29%|██▊       | 2/7 [00:01<00:02,  1.81it/s] 43%|████▎     | 3/7 [00:01<00:02,  1.83it/s] 57%|█████▋    | 4/7 [00:02<00:01,  1.80it/s] 71%|███████▏  | 5/7 [00:02<00:01,  1.76it/s] 86%|████████▌ | 6/7 [00:03<00:00,  1.78it/s]100%|██████████| 7/7 [00:03<00:00,  2.27it/s]100%|██████████| 7/7 [00:03<00:00,  1.97it/s]
all_samples: 203
list_of_results: 203
global MRR: 0.7713821213512807
global Precision at 10: 0.7980295566502463
global Precision at 1: 0.7586206896551724


P@1 : 0.7586206896551724

{'description': 'specialization of a '
                'person or '
                'organization; see P106 '
                'for the occupation',
 'label': 'field of work',
 'relation': 'P101',
 'template': '[X] works in the field of '
             '[Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P101.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] works in the field of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P101', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
696
573
distinct template facts: 573
  0%|          | 0/18 [00:00<?, ?it/s]  6%|▌         | 1/18 [00:00<00:09,  1.75it/s] 11%|█         | 2/18 [00:01<00:09,  1.76it/s] 17%|█▋        | 3/18 [00:01<00:08,  1.75it/s] 22%|██▏       | 4/18 [00:02<00:08,  1.73it/s] 28%|██▊       | 5/18 [00:02<00:07,  1.74it/s] 33%|███▎      | 6/18 [00:03<00:06,  1.76it/s] 39%|███▉      | 7/18 [00:04<00:06,  1.75it/s] 44%|████▍     | 8/18 [00:04<00:05,  1.77it/s] 50%|█████     | 9/18 [00:05<00:05,  1.76it/s] 56%|█████▌    | 10/18 [00:05<00:04,  1.76it/s] 61%|██████    | 11/18 [00:06<00:04,  1.75it/s] 67%|██████▋   | 12/18 [00:06<00:03,  1.69it/s] 72%|███████▏  | 13/18 [00:07<00:03,  1.66it/s] 78%|███████▊  | 14/18 [00:08<00:02,  1.64it/s] 83%|████████▎ | 15/18 [00:08<00:01,  1.62it/s] 89%|████████▉ | 16/18 [00:09<00:01,  1.57it/s] 94%|█████████▍| 17/18 [00:10<00:00,  1.59it/s]100%|██████████| 18/18 [00:10<00:00,  1.65it/s]100%|██████████| 18/18 [00:10<00:00,  1.69it/s]
all_samples: 573
list_of_results: 573
global MRR: 0.23732550090085888
global Precision at 10: 0.4537521815008726
global Precision at 1: 0.13961605584642234


P@1 : 0.13961605584642234

{'description': "Like 'Participant' "
                '(P710) but for teams. '
                'For an event like a '
                'cycle race or a '
                'football match you can '
                'use this property to '
                'list the teams and '
                'P710 to list the '
                'individuals (with '
                "'member of sports "
                "team' (P54)' as a "
                'qualifier for the '
                'individuals)',
 'label': 'participating team',
 'relation': 'P1923',
 'template': '[Y] participated in the '
             '[X] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P1923.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[Y] participated in the [X] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P1923', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Relation P1923 excluded.
Exception: [Errno 2] No such file or directory: 'data/TREx/P1923.jsonl'
{'description': 'occupation of a '
                'person; see also '
                '"field of work" '
                '(Property:P101), '
                '"position held" '
                '(Property:P39)',
 'label': 'occupation',
 'relation': 'P106',
 'template': '[X] is a [Y] by '
             'profession .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P106.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a [Y] by profession .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P106', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
958
821
distinct template facts: 821
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:00<00:16,  1.53it/s]  8%|▊         | 2/26 [00:01<00:14,  1.60it/s] 12%|█▏        | 3/26 [00:01<00:14,  1.63it/s] 15%|█▌        | 4/26 [00:02<00:13,  1.64it/s] 19%|█▉        | 5/26 [00:03<00:12,  1.65it/s] 23%|██▎       | 6/26 [00:03<00:12,  1.63it/s] 27%|██▋       | 7/26 [00:04<00:11,  1.68it/s] 31%|███       | 8/26 [00:04<00:10,  1.67it/s] 35%|███▍      | 9/26 [00:05<00:10,  1.64it/s] 38%|███▊      | 10/26 [00:06<00:09,  1.65it/s] 42%|████▏     | 11/26 [00:06<00:08,  1.69it/s] 46%|████▌     | 12/26 [00:07<00:08,  1.73it/s] 50%|█████     | 13/26 [00:07<00:07,  1.74it/s] 54%|█████▍    | 14/26 [00:08<00:06,  1.77it/s] 58%|█████▊    | 15/26 [00:08<00:06,  1.77it/s] 62%|██████▏   | 16/26 [00:09<00:05,  1.76it/s] 65%|██████▌   | 17/26 [00:09<00:05,  1.77it/s] 69%|██████▉   | 18/26 [00:10<00:04,  1.78it/s] 73%|███████▎  | 19/26 [00:11<00:03,  1.79it/s] 77%|███████▋  | 20/26 [00:11<00:03,  1.80it/s] 81%|████████  | 21/26 [00:12<00:02,  1.80it/s] 85%|████████▍ | 22/26 [00:12<00:02,  1.79it/s] 88%|████████▊ | 23/26 [00:13<00:01,  1.78it/s] 92%|█████████▏| 24/26 [00:13<00:01,  1.73it/s] 96%|█████████▌| 25/26 [00:14<00:00,  1.67it/s]100%|██████████| 26/26 [00:15<00:00,  1.84it/s]100%|██████████| 26/26 [00:15<00:00,  1.73it/s]
all_samples: 821
list_of_results: 821
global MRR: 0.055464987229661915
global Precision at 10: 0.10475030450669914
global Precision at 1: 0.0060901339829476245


P@1 : 0.0060901339829476245

{'description': 'part of this subject; '
                'inverse property of '
                '"part of" (P361). See '
                'also "has parts of the '
                'class" (P2670).',
 'label': 'has part',
 'relation': 'P527',
 'template': '[X] consists of [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P527.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] consists of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P527', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
976
956
distinct template facts: 956
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:16,  1.71it/s]  7%|▋         | 2/30 [00:01<00:15,  1.78it/s] 10%|█         | 3/30 [00:01<00:15,  1.79it/s] 13%|█▎        | 4/30 [00:02<00:14,  1.81it/s] 17%|█▋        | 5/30 [00:02<00:13,  1.82it/s] 20%|██        | 6/30 [00:03<00:13,  1.82it/s] 23%|██▎       | 7/30 [00:03<00:12,  1.82it/s] 27%|██▋       | 8/30 [00:04<00:12,  1.80it/s] 30%|███       | 9/30 [00:05<00:12,  1.68it/s] 33%|███▎      | 10/30 [00:05<00:11,  1.70it/s] 37%|███▋      | 11/30 [00:06<00:11,  1.67it/s] 40%|████      | 12/30 [00:06<00:10,  1.65it/s] 43%|████▎     | 13/30 [00:07<00:10,  1.65it/s] 47%|████▋     | 14/30 [00:08<00:09,  1.68it/s] 50%|█████     | 15/30 [00:08<00:08,  1.69it/s] 53%|█████▎    | 16/30 [00:09<00:08,  1.56it/s] 57%|█████▋    | 17/30 [00:10<00:07,  1.63it/s] 60%|██████    | 18/30 [00:10<00:07,  1.68it/s] 63%|██████▎   | 19/30 [00:11<00:06,  1.73it/s] 67%|██████▋   | 20/30 [00:11<00:05,  1.76it/s] 70%|███████   | 21/30 [00:12<00:05,  1.78it/s] 73%|███████▎  | 22/30 [00:12<00:04,  1.72it/s] 77%|███████▋  | 23/30 [00:13<00:04,  1.69it/s] 80%|████████  | 24/30 [00:14<00:03,  1.67it/s] 83%|████████▎ | 25/30 [00:14<00:02,  1.72it/s] 87%|████████▋ | 26/30 [00:15<00:02,  1.70it/s] 90%|█████████ | 27/30 [00:15<00:01,  1.73it/s] 93%|█████████▎| 28/30 [00:16<00:01,  1.75it/s] 97%|█████████▋| 29/30 [00:16<00:00,  1.78it/s]100%|██████████| 30/30 [00:17<00:00,  1.86it/s]100%|██████████| 30/30 [00:17<00:00,  1.73it/s]
all_samples: 956
list_of_results: 956
global MRR: 0.1326529147571951
global Precision at 10: 0.2939330543933054
global Precision at 1: 0.060669456066945605


P@1 : 0.060669456066945605

{'description': 'the political party of '
                'which this politician '
                'is or has been a '
                'member',
 'label': 'member of political party',
 'relation': 'P102',
 'template': '[X] is a member of the '
             '[Y] political party .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P102.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a member of the [Y] political party .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P102', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Relation P102 excluded.
Exception: [Errno 2] No such file or directory: 'data/TREx/P102.jsonl'
{'description': 'diplomatic relations '
                'of the country',
 'label': 'diplomatic relation',
 'relation': 'P530',
 'template': '[X] maintains diplomatic '
             'relations with [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P530.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] maintains diplomatic relations with [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P530', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
996
950
distinct template facts: 950
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:17,  1.69it/s]  7%|▋         | 2/30 [00:01<00:16,  1.73it/s] 10%|█         | 3/30 [00:01<00:15,  1.78it/s] 13%|█▎        | 4/30 [00:02<00:14,  1.83it/s] 17%|█▋        | 5/30 [00:02<00:13,  1.86it/s] 20%|██        | 6/30 [00:03<00:12,  1.87it/s] 23%|██▎       | 7/30 [00:03<00:12,  1.89it/s] 27%|██▋       | 8/30 [00:04<00:11,  1.86it/s] 30%|███       | 9/30 [00:04<00:11,  1.83it/s] 33%|███▎      | 10/30 [00:05<00:11,  1.78it/s] 37%|███▋      | 11/30 [00:06<00:10,  1.82it/s] 40%|████      | 12/30 [00:06<00:09,  1.85it/s] 43%|████▎     | 13/30 [00:07<00:09,  1.71it/s] 47%|████▋     | 14/30 [00:07<00:09,  1.75it/s] 50%|█████     | 15/30 [00:08<00:08,  1.79it/s] 53%|█████▎    | 16/30 [00:08<00:07,  1.82it/s] 57%|█████▋    | 17/30 [00:09<00:07,  1.84it/s] 60%|██████    | 18/30 [00:09<00:06,  1.86it/s] 63%|██████▎   | 19/30 [00:10<00:05,  1.87it/s] 67%|██████▋   | 20/30 [00:10<00:05,  1.88it/s] 70%|███████   | 21/30 [00:11<00:04,  1.88it/s] 73%|███████▎  | 22/30 [00:11<00:04,  1.90it/s] 77%|███████▋  | 23/30 [00:12<00:03,  1.85it/s] 80%|████████  | 24/30 [00:13<00:03,  1.86it/s] 83%|████████▎ | 25/30 [00:13<00:02,  1.87it/s] 87%|████████▋ | 26/30 [00:14<00:02,  1.85it/s] 90%|█████████ | 27/30 [00:14<00:01,  1.87it/s] 93%|█████████▎| 28/30 [00:15<00:01,  1.88it/s] 97%|█████████▋| 29/30 [00:15<00:00,  1.87it/s]100%|██████████| 30/30 [00:16<00:00,  1.97it/s]100%|██████████| 30/30 [00:16<00:00,  1.85it/s]
all_samples: 950
list_of_results: 950
global MRR: 0.08854598699918872
global Precision at 10: 0.19789473684210526
global Precision at 1: 0.023157894736842106


P@1 : 0.023157894736842106

{'description': 'manufacturer or '
                'producer of this '
                'product',
 'label': 'manufacturer',
 'relation': 'P176',
 'template': '[X] is produced by [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P176.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is produced by [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P176', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
982
957
distinct template facts: 948
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:17,  1.62it/s]  7%|▋         | 2/30 [00:01<00:16,  1.66it/s] 10%|█         | 3/30 [00:01<00:15,  1.74it/s] 13%|█▎        | 4/30 [00:02<00:14,  1.78it/s] 17%|█▋        | 5/30 [00:02<00:13,  1.80it/s] 20%|██        | 6/30 [00:03<00:13,  1.77it/s] 23%|██▎       | 7/30 [00:03<00:12,  1.80it/s] 27%|██▋       | 8/30 [00:04<00:12,  1.81it/s] 30%|███       | 9/30 [00:05<00:11,  1.83it/s] 33%|███▎      | 10/30 [00:05<00:10,  1.85it/s] 37%|███▋      | 11/30 [00:06<00:10,  1.85it/s] 40%|████      | 12/30 [00:06<00:10,  1.78it/s] 43%|████▎     | 13/30 [00:07<00:09,  1.79it/s] 47%|████▋     | 14/30 [00:07<00:08,  1.80it/s] 50%|█████     | 15/30 [00:08<00:08,  1.80it/s] 53%|█████▎    | 16/30 [00:08<00:07,  1.80it/s] 57%|█████▋    | 17/30 [00:09<00:07,  1.82it/s] 60%|██████    | 18/30 [00:10<00:07,  1.64it/s] 63%|██████▎   | 19/30 [00:10<00:06,  1.71it/s] 67%|██████▋   | 20/30 [00:11<00:05,  1.72it/s] 70%|███████   | 21/30 [00:11<00:05,  1.77it/s] 73%|███████▎  | 22/30 [00:12<00:04,  1.80it/s] 77%|███████▋  | 23/30 [00:12<00:03,  1.80it/s] 80%|████████  | 24/30 [00:13<00:03,  1.76it/s] 83%|████████▎ | 25/30 [00:14<00:02,  1.75it/s] 87%|████████▋ | 26/30 [00:14<00:02,  1.78it/s] 90%|█████████ | 27/30 [00:15<00:01,  1.79it/s] 93%|█████████▎| 28/30 [00:15<00:01,  1.81it/s] 97%|█████████▋| 29/30 [00:16<00:00,  1.82it/s]100%|██████████| 30/30 [00:16<00:00,  2.06it/s]100%|██████████| 30/30 [00:16<00:00,  1.80it/s]
all_samples: 948
list_of_results: 948
global MRR: 0.746083812043783
global Precision at 10: 0.8491561181434599
global Precision at 1: 0.6983122362869199


P@1 : 0.6983122362869199

{'description': 'the object is a '
                'country that '
                'recognizes the subject '
                'as its citizen',
 'label': 'country of citizenship',
 'relation': 'P27',
 'template': '[X] is [Y] citizen .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P27.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is [Y] citizen .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P27', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
966
958
distinct template facts: 958
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:16,  1.75it/s]  7%|▋         | 2/30 [00:01<00:16,  1.74it/s] 10%|█         | 3/30 [00:01<00:15,  1.79it/s] 13%|█▎        | 4/30 [00:02<00:14,  1.82it/s] 17%|█▋        | 5/30 [00:02<00:13,  1.81it/s] 20%|██        | 6/30 [00:03<00:13,  1.81it/s] 23%|██▎       | 7/30 [00:03<00:12,  1.82it/s] 27%|██▋       | 8/30 [00:04<00:12,  1.83it/s] 30%|███       | 9/30 [00:04<00:11,  1.83it/s] 33%|███▎      | 10/30 [00:05<00:11,  1.81it/s] 37%|███▋      | 11/30 [00:06<00:10,  1.81it/s] 40%|████      | 12/30 [00:06<00:09,  1.80it/s] 43%|████▎     | 13/30 [00:07<00:09,  1.75it/s] 47%|████▋     | 14/30 [00:07<00:09,  1.73it/s] 50%|█████     | 15/30 [00:08<00:08,  1.71it/s] 53%|█████▎    | 16/30 [00:09<00:08,  1.73it/s] 57%|█████▋    | 17/30 [00:09<00:07,  1.77it/s] 60%|██████    | 18/30 [00:10<00:06,  1.80it/s] 63%|██████▎   | 19/30 [00:10<00:06,  1.82it/s] 67%|██████▋   | 20/30 [00:11<00:05,  1.79it/s] 70%|███████   | 21/30 [00:11<00:04,  1.81it/s] 73%|███████▎  | 22/30 [00:12<00:04,  1.84it/s] 77%|███████▋  | 23/30 [00:13<00:04,  1.62it/s] 80%|████████  | 24/30 [00:13<00:03,  1.69it/s] 83%|████████▎ | 25/30 [00:14<00:02,  1.74it/s] 87%|████████▋ | 26/30 [00:14<00:02,  1.77it/s] 90%|█████████ | 27/30 [00:15<00:01,  1.78it/s] 93%|█████████▎| 28/30 [00:15<00:01,  1.79it/s] 97%|█████████▋| 29/30 [00:16<00:00,  1.80it/s]100%|██████████| 30/30 [00:16<00:00,  1.83it/s]100%|██████████| 30/30 [00:16<00:00,  1.78it/s]
all_samples: 958
list_of_results: 958
global MRR: 0.01965633264059148
global Precision at 10: 0.04279749478079332
global Precision at 1: 0.0


P@1 : 0.0

{'description': 'language associated '
                'with this creative '
                'work (such as books, '
                'shows, songs, or '
                'websites) or a name '
                '(for persons use P103 '
                'and P1412)',
 'label': 'language of work or name',
 'relation': 'P407',
 'template': '[X] was written in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P407.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was written in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P407', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
877
857
distinct template facts: 857
  0%|          | 0/27 [00:00<?, ?it/s]  4%|▎         | 1/27 [00:00<00:14,  1.77it/s]  7%|▋         | 2/27 [00:01<00:13,  1.81it/s] 11%|█         | 3/27 [00:01<00:13,  1.84it/s] 15%|█▍        | 4/27 [00:02<00:12,  1.81it/s] 19%|█▊        | 5/27 [00:02<00:12,  1.81it/s] 22%|██▏       | 6/27 [00:03<00:11,  1.83it/s] 26%|██▌       | 7/27 [00:03<00:10,  1.84it/s] 30%|██▉       | 8/27 [00:04<00:10,  1.74it/s] 33%|███▎      | 9/27 [00:05<00:10,  1.69it/s] 37%|███▋      | 10/27 [00:05<00:10,  1.66it/s] 41%|████      | 11/27 [00:06<00:09,  1.66it/s] 44%|████▍     | 12/27 [00:06<00:08,  1.68it/s] 48%|████▊     | 13/27 [00:07<00:08,  1.72it/s] 52%|█████▏    | 14/27 [00:07<00:07,  1.76it/s] 56%|█████▌    | 15/27 [00:08<00:06,  1.78it/s] 59%|█████▉    | 16/27 [00:09<00:06,  1.79it/s] 63%|██████▎   | 17/27 [00:09<00:05,  1.80it/s] 67%|██████▋   | 18/27 [00:10<00:05,  1.80it/s] 70%|███████   | 19/27 [00:10<00:04,  1.75it/s] 74%|███████▍  | 20/27 [00:11<00:04,  1.73it/s] 78%|███████▊  | 21/27 [00:12<00:03,  1.70it/s] 81%|████████▏ | 22/27 [00:12<00:02,  1.71it/s] 85%|████████▌ | 23/27 [00:13<00:02,  1.73it/s] 89%|████████▉ | 24/27 [00:13<00:01,  1.73it/s] 93%|█████████▎| 25/27 [00:14<00:01,  1.74it/s] 96%|█████████▋| 26/27 [00:14<00:00,  1.75it/s]100%|██████████| 27/27 [00:15<00:00,  1.88it/s]100%|██████████| 27/27 [00:15<00:00,  1.76it/s]
all_samples: 857
list_of_results: 857
global MRR: 0.4059251927684969
global Precision at 10: 0.5332555425904317
global Precision at 1: 0.3267211201866978


P@1 : 0.3267211201866978

{'description': 'continent of which the '
                'subject is a part',
 'label': 'continent',
 'relation': 'P30',
 'template': '[X] is located in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P30.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is located in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P30', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
975
959
distinct template facts: 959
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:00<00:15,  1.83it/s]  7%|▋         | 2/30 [00:01<00:15,  1.80it/s] 10%|█         | 3/30 [00:01<00:14,  1.82it/s] 13%|█▎        | 4/30 [00:02<00:14,  1.77it/s] 17%|█▋        | 5/30 [00:02<00:14,  1.75it/s] 20%|██        | 6/30 [00:03<00:13,  1.73it/s] 23%|██▎       | 7/30 [00:04<00:13,  1.72it/s] 27%|██▋       | 8/30 [00:04<00:12,  1.75it/s] 30%|███       | 9/30 [00:05<00:11,  1.77it/s] 33%|███▎      | 10/30 [00:05<00:11,  1.81it/s] 37%|███▋      | 11/30 [00:06<00:10,  1.82it/s] 40%|████      | 12/30 [00:06<00:09,  1.84it/s] 43%|████▎     | 13/30 [00:07<00:09,  1.84it/s] 47%|████▋     | 14/30 [00:07<00:08,  1.84it/s] 50%|█████     | 15/30 [00:08<00:08,  1.83it/s] 53%|█████▎    | 16/30 [00:08<00:07,  1.79it/s] 57%|█████▋    | 17/30 [00:09<00:07,  1.77it/s] 60%|██████    | 18/30 [00:10<00:06,  1.74it/s] 63%|██████▎   | 19/30 [00:10<00:06,  1.74it/s] 67%|██████▋   | 20/30 [00:11<00:05,  1.73it/s] 70%|███████   | 21/30 [00:11<00:05,  1.70it/s] 73%|███████▎  | 22/30 [00:12<00:04,  1.69it/s] 77%|███████▋  | 23/30 [00:13<00:04,  1.72it/s] 80%|████████  | 24/30 [00:13<00:03,  1.73it/s] 83%|████████▎ | 25/30 [00:14<00:02,  1.73it/s] 87%|████████▋ | 26/30 [00:14<00:02,  1.74it/s] 90%|█████████ | 27/30 [00:15<00:01,  1.75it/s] 93%|█████████▎| 28/30 [00:15<00:01,  1.75it/s] 97%|█████████▋| 29/30 [00:16<00:00,  1.74it/s]100%|██████████| 30/30 [00:17<00:00,  1.76it/s]100%|██████████| 30/30 [00:17<00:00,  1.76it/s]
all_samples: 959
list_of_results: 959
global MRR: 0.26933807047171204
global Precision at 10: 0.44004171011470283
global Precision at 1: 0.18248175182481752


P@1 : 0.18248175182481752

{'description': 'organisation or person '
                'that developed the '
                'item',
 'label': 'developer',
 'relation': 'P178',
 'template': '[X] is developed by [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P178.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is developed by [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P178', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
592
588
distinct template facts: 587
  0%|          | 0/19 [00:00<?, ?it/s]  5%|▌         | 1/19 [00:00<00:12,  1.48it/s] 11%|█         | 2/19 [00:01<00:10,  1.67it/s] 16%|█▌        | 3/19 [00:01<00:09,  1.77it/s] 21%|██        | 4/19 [00:02<00:08,  1.77it/s] 26%|██▋       | 5/19 [00:02<00:07,  1.78it/s] 32%|███▏      | 6/19 [00:03<00:07,  1.80it/s] 37%|███▋      | 7/19 [00:03<00:06,  1.81it/s] 42%|████▏     | 8/19 [00:04<00:05,  1.84it/s] 47%|████▋     | 9/19 [00:05<00:05,  1.85it/s] 53%|█████▎    | 10/19 [00:05<00:04,  1.86it/s] 58%|█████▊    | 11/19 [00:06<00:04,  1.88it/s] 63%|██████▎   | 12/19 [00:06<00:03,  1.88it/s] 68%|██████▊   | 13/19 [00:07<00:03,  1.88it/s] 74%|███████▎  | 14/19 [00:07<00:02,  1.88it/s] 79%|███████▉  | 15/19 [00:08<00:02,  1.88it/s] 84%|████████▍ | 16/19 [00:08<00:01,  1.88it/s] 89%|████████▉ | 17/19 [00:09<00:01,  1.87it/s] 95%|█████████▍| 18/19 [00:09<00:00,  1.85it/s]100%|██████████| 19/19 [00:10<00:00,  2.31it/s]100%|██████████| 19/19 [00:10<00:00,  1.90it/s]
all_samples: 587
list_of_results: 587
global MRR: 0.6604908797074044
global Precision at 10: 0.8245315161839863
global Precision at 1: 0.5792163543441227


P@1 : 0.5792163543441227

{'description': 'country, state, '
                'department, canton or '
                'other administrative '
                'division of which the '
                'municipality is the '
                'governmental seat',
 'label': 'capital of',
 'relation': 'P1376',
 'template': '[X] is the capital of [Y] '
             '.',
 'type': '1-1'}
{'dataset_filename': 'data/TREx/P1376.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is the capital of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P1376', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
234
180
distinct template facts: 179
  0%|          | 0/6 [00:00<?, ?it/s] 17%|█▋        | 1/6 [00:00<00:02,  1.73it/s] 33%|███▎      | 2/6 [00:01<00:02,  1.80it/s] 50%|█████     | 3/6 [00:01<00:01,  1.83it/s] 67%|██████▋   | 4/6 [00:02<00:01,  1.76it/s] 83%|████████▎ | 5/6 [00:02<00:00,  1.78it/s]100%|██████████| 6/6 [00:03<00:00,  2.06it/s]100%|██████████| 6/6 [00:03<00:00,  1.91it/s]
all_samples: 179
list_of_results: 179
global MRR: 0.6551137613902107
global Precision at 10: 0.8100558659217877
global Precision at 1: 0.5754189944134078


P@1 : 0.5754189944134078

{'description': 'the item is located on '
                'the territory of the '
                'following '
                'administrative entity. '
                'Use P276 (location) '
                'for specifying the '
                'location of '
                'non-administrative '
                'places and for items '
                'about events',
 'label': 'located in the '
          'administrative territorial '
          'entity',
 'relation': 'P131',
 'template': '[X] is located in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P131.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is located in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P131', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
881
775
distinct template facts: 775
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:00<00:13,  1.78it/s]  8%|▊         | 2/25 [00:01<00:12,  1.82it/s] 12%|█▏        | 3/25 [00:01<00:12,  1.83it/s] 16%|█▌        | 4/25 [00:02<00:11,  1.83it/s] 20%|██        | 5/25 [00:02<00:10,  1.83it/s] 24%|██▍       | 6/25 [00:03<00:10,  1.84it/s] 28%|██▊       | 7/25 [00:03<00:10,  1.73it/s] 32%|███▏      | 8/25 [00:04<00:09,  1.77it/s] 36%|███▌      | 9/25 [00:05<00:08,  1.78it/s] 40%|████      | 10/25 [00:05<00:08,  1.79it/s] 44%|████▍     | 11/25 [00:06<00:07,  1.81it/s] 48%|████▊     | 12/25 [00:06<00:07,  1.82it/s] 52%|█████▏    | 13/25 [00:07<00:06,  1.82it/s] 56%|█████▌    | 14/25 [00:07<00:06,  1.81it/s] 60%|██████    | 15/25 [00:08<00:05,  1.80it/s] 64%|██████▍   | 16/25 [00:08<00:04,  1.82it/s] 68%|██████▊   | 17/25 [00:09<00:04,  1.83it/s] 72%|███████▏  | 18/25 [00:09<00:03,  1.84it/s] 76%|███████▌  | 19/25 [00:10<00:03,  1.84it/s] 80%|████████  | 20/25 [00:11<00:02,  1.85it/s] 84%|████████▍ | 21/25 [00:11<00:02,  1.84it/s] 88%|████████▊ | 22/25 [00:12<00:01,  1.75it/s] 92%|█████████▏| 23/25 [00:12<00:01,  1.77it/s] 96%|█████████▌| 24/25 [00:13<00:00,  1.74it/s]100%|██████████| 25/25 [00:13<00:00,  2.26it/s]100%|██████████| 25/25 [00:13<00:00,  1.85it/s]
all_samples: 775
list_of_results: 775
global MRR: 0.33438114241994976
global Precision at 10: 0.56
global Precision at 1: 0.21935483870967742


P@1 : 0.21935483870967742

{'description': 'language(s) that a '
                'person speaks or '
                'writes, including the '
                'native language(s)',
 'label': 'languages spoken, written or '
          'signed',
 'relation': 'P1412',
 'template': '[X] used to communicate '
             'in [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P1412.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] used to communicate in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P1412', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
969
924
distinct template facts: 924
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:17,  1.59it/s]  7%|▋         | 2/29 [00:01<00:16,  1.60it/s] 10%|█         | 3/29 [00:01<00:15,  1.68it/s] 14%|█▍        | 4/29 [00:02<00:14,  1.72it/s] 17%|█▋        | 5/29 [00:02<00:13,  1.76it/s] 21%|██        | 6/29 [00:03<00:12,  1.77it/s] 24%|██▍       | 7/29 [00:04<00:12,  1.79it/s] 28%|██▊       | 8/29 [00:04<00:11,  1.80it/s] 31%|███       | 9/29 [00:05<00:10,  1.82it/s] 34%|███▍      | 10/29 [00:05<00:10,  1.82it/s] 38%|███▊      | 11/29 [00:06<00:10,  1.78it/s] 41%|████▏     | 12/29 [00:06<00:09,  1.80it/s] 45%|████▍     | 13/29 [00:07<00:08,  1.80it/s] 48%|████▊     | 14/29 [00:07<00:08,  1.81it/s] 52%|█████▏    | 15/29 [00:08<00:07,  1.82it/s] 55%|█████▌    | 16/29 [00:08<00:07,  1.83it/s] 59%|█████▊    | 17/29 [00:09<00:07,  1.64it/s] 62%|██████▏   | 18/29 [00:10<00:06,  1.69it/s] 66%|██████▌   | 19/29 [00:10<00:05,  1.74it/s] 69%|██████▉   | 20/29 [00:11<00:05,  1.74it/s] 72%|███████▏  | 21/29 [00:11<00:04,  1.73it/s] 76%|███████▌  | 22/29 [00:12<00:04,  1.71it/s] 79%|███████▉  | 23/29 [00:13<00:03,  1.72it/s] 83%|████████▎ | 24/29 [00:13<00:02,  1.72it/s] 86%|████████▌ | 25/29 [00:14<00:02,  1.72it/s] 90%|████████▉ | 26/29 [00:14<00:01,  1.72it/s] 93%|█████████▎| 27/29 [00:15<00:01,  1.73it/s] 97%|█████████▋| 28/29 [00:16<00:00,  1.71it/s]100%|██████████| 29/29 [00:16<00:00,  1.78it/s]100%|██████████| 29/29 [00:16<00:00,  1.75it/s]
all_samples: 924
list_of_results: 924
global MRR: 0.6660355011830885
global Precision at 10: 0.8982683982683982
global Precision at 1: 0.5378787878787878


P@1 : 0.5378787878787878

{'description': 'person or organization '
                'for which the subject '
                'works or worked',
 'label': 'employer',
 'relation': 'P108',
 'template': '[X] works for [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P108.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] works for [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P108', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
383
378
distinct template facts: 378
  0%|          | 0/12 [00:00<?, ?it/s]  8%|▊         | 1/12 [00:00<00:06,  1.79it/s] 17%|█▋        | 2/12 [00:01<00:05,  1.83it/s] 25%|██▌       | 3/12 [00:01<00:04,  1.84it/s] 33%|███▎      | 4/12 [00:02<00:04,  1.86it/s] 42%|████▏     | 5/12 [00:02<00:03,  1.86it/s] 50%|█████     | 6/12 [00:03<00:03,  1.87it/s] 58%|█████▊    | 7/12 [00:03<00:02,  1.87it/s] 67%|██████▋   | 8/12 [00:04<00:02,  1.86it/s] 75%|███████▌  | 9/12 [00:04<00:01,  1.86it/s] 83%|████████▎ | 10/12 [00:05<00:01,  1.79it/s] 92%|█████████▏| 11/12 [00:06<00:00,  1.78it/s]100%|██████████| 12/12 [00:06<00:00,  1.89it/s]100%|██████████| 12/12 [00:06<00:00,  1.85it/s]
all_samples: 378
list_of_results: 378
global MRR: 0.14527612581755658
global Precision at 10: 0.2671957671957672
global Precision at 1: 0.06349206349206349


P@1 : 0.06349206349206349

{'description': "creative work's genre "
                "or an artist's field "
                'of work (P101). Use '
                'main subject (P921) to '
                'relate creative works '
                'to their topic',
 'label': 'genre',
 'relation': 'P136',
 'template': '[X] plays [Y] music .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P136.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] plays [Y] music .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P136', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
931
859
distinct template facts: 859
  0%|          | 0/27 [00:00<?, ?it/s]  4%|▎         | 1/27 [00:00<00:14,  1.82it/s]  7%|▋         | 2/27 [00:01<00:13,  1.81it/s] 11%|█         | 3/27 [00:01<00:13,  1.75it/s] 15%|█▍        | 4/27 [00:02<00:13,  1.68it/s] 19%|█▊        | 5/27 [00:03<00:14,  1.57it/s] 22%|██▏       | 6/27 [00:03<00:12,  1.63it/s] 26%|██▌       | 7/27 [00:04<00:11,  1.68it/s] 30%|██▉       | 8/27 [00:04<00:11,  1.71it/s] 33%|███▎      | 9/27 [00:05<00:10,  1.74it/s] 37%|███▋      | 10/27 [00:05<00:09,  1.76it/s] 41%|████      | 11/27 [00:06<00:09,  1.71it/s] 44%|████▍     | 12/27 [00:06<00:08,  1.75it/s] 48%|████▊     | 13/27 [00:07<00:07,  1.77it/s] 52%|█████▏    | 14/27 [00:08<00:07,  1.79it/s] 56%|█████▌    | 15/27 [00:08<00:06,  1.80it/s] 59%|█████▉    | 16/27 [00:09<00:06,  1.80it/s] 63%|██████▎   | 17/27 [00:09<00:05,  1.81it/s] 67%|██████▋   | 18/27 [00:10<00:04,  1.82it/s] 70%|███████   | 19/27 [00:10<00:04,  1.82it/s] 74%|███████▍  | 20/27 [00:11<00:03,  1.82it/s] 78%|███████▊  | 21/27 [00:11<00:03,  1.82it/s] 81%|████████▏ | 22/27 [00:12<00:02,  1.82it/s] 85%|████████▌ | 23/27 [00:13<00:02,  1.80it/s] 89%|████████▉ | 24/27 [00:13<00:01,  1.79it/s] 93%|█████████▎| 25/27 [00:14<00:01,  1.78it/s] 96%|█████████▋| 26/27 [00:14<00:00,  1.68it/s]100%|██████████| 27/27 [00:15<00:00,  1.75it/s]100%|██████████| 27/27 [00:15<00:00,  1.76it/s]
all_samples: 859
list_of_results: 859
global MRR: 0.07883890191129063
global Precision at 10: 0.36321303841676367
global Precision at 1: 0.0034924330616996507


P@1 : 0.0034924330616996507

{'description': 'sovereign state of '
                "this item; don't use "
                'on humans',
 'label': 'country',
 'relation': 'P17',
 'template': '[X] is located in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P17.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is located in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P17', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
930
912
distinct template facts: 912
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:17,  1.59it/s]  7%|▋         | 2/29 [00:01<00:16,  1.67it/s] 10%|█         | 3/29 [00:01<00:14,  1.74it/s] 14%|█▍        | 4/29 [00:02<00:14,  1.78it/s] 17%|█▋        | 5/29 [00:02<00:13,  1.80it/s] 21%|██        | 6/29 [00:03<00:12,  1.82it/s] 24%|██▍       | 7/29 [00:03<00:11,  1.83it/s] 28%|██▊       | 8/29 [00:04<00:11,  1.84it/s] 31%|███       | 9/29 [00:05<00:11,  1.79it/s] 34%|███▍      | 10/29 [00:05<00:10,  1.81it/s] 38%|███▊      | 11/29 [00:06<00:10,  1.80it/s] 41%|████▏     | 12/29 [00:06<00:09,  1.80it/s] 45%|████▍     | 13/29 [00:07<00:09,  1.69it/s] 48%|████▊     | 14/29 [00:07<00:08,  1.70it/s] 52%|█████▏    | 15/29 [00:08<00:08,  1.74it/s] 55%|█████▌    | 16/29 [00:09<00:07,  1.76it/s] 59%|█████▊    | 17/29 [00:09<00:06,  1.75it/s] 62%|██████▏   | 18/29 [00:10<00:06,  1.70it/s] 66%|██████▌   | 19/29 [00:10<00:05,  1.68it/s] 69%|██████▉   | 20/29 [00:11<00:05,  1.67it/s] 72%|███████▏  | 21/29 [00:12<00:04,  1.66it/s] 76%|███████▌  | 22/29 [00:12<00:04,  1.65it/s] 79%|███████▉  | 23/29 [00:13<00:03,  1.65it/s] 83%|████████▎ | 24/29 [00:13<00:02,  1.67it/s] 86%|████████▌ | 25/29 [00:14<00:02,  1.66it/s] 90%|████████▉ | 26/29 [00:15<00:01,  1.65it/s] 93%|█████████▎| 27/29 [00:15<00:01,  1.66it/s] 97%|█████████▋| 28/29 [00:16<00:00,  1.63it/s]100%|██████████| 29/29 [00:16<00:00,  1.90it/s]100%|██████████| 29/29 [00:16<00:00,  1.74it/s]
all_samples: 912
list_of_results: 912
global MRR: 0.33254919406720496
global Precision at 10: 0.5712719298245614
global Precision at 1: 0.2138157894736842


P@1 : 0.2138157894736842

{'description': 'subject currently or '
                'formerly holds the '
                'object position or '
                'public office',
 'label': 'position held',
 'relation': 'P39',
 'template': '[X] has the position of '
             '[Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P39.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] has the position of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P39', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
892
485
distinct template facts: 485
  0%|          | 0/16 [00:00<?, ?it/s]  6%|▋         | 1/16 [00:00<00:09,  1.65it/s] 12%|█▎        | 2/16 [00:01<00:08,  1.62it/s] 19%|█▉        | 3/16 [00:01<00:07,  1.68it/s] 25%|██▌       | 4/16 [00:02<00:07,  1.71it/s] 31%|███▏      | 5/16 [00:02<00:06,  1.73it/s] 38%|███▊      | 6/16 [00:03<00:05,  1.73it/s] 44%|████▍     | 7/16 [00:04<00:05,  1.71it/s] 50%|█████     | 8/16 [00:04<00:04,  1.73it/s] 56%|█████▋    | 9/16 [00:05<00:04,  1.74it/s] 62%|██████▎   | 10/16 [00:05<00:03,  1.75it/s] 69%|██████▉   | 11/16 [00:06<00:02,  1.71it/s] 75%|███████▌  | 12/16 [00:07<00:02,  1.72it/s] 81%|████████▏ | 13/16 [00:07<00:01,  1.74it/s] 88%|████████▊ | 14/16 [00:08<00:01,  1.72it/s] 94%|█████████▍| 15/16 [00:08<00:00,  1.69it/s]100%|██████████| 16/16 [00:08<00:00,  2.25it/s]100%|██████████| 16/16 [00:08<00:00,  1.80it/s]
all_samples: 485
list_of_results: 485
global MRR: 0.10446445927887969
global Precision at 10: 0.24123711340206186
global Precision at 1: 0.04742268041237113


P@1 : 0.04742268041237113

{'description': 'brand and trademark '
                'associated with the '
                'marketing of subject '
                'music recordings and '
                'music videos',
 'label': 'record label',
 'relation': 'P264',
 'template': '[X] is represented by '
             'music label [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P264.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is represented by music label [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P264', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
429
53
distinct template facts: 53
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:00<00:00,  1.60it/s]100%|██████████| 2/2 [00:00<00:00,  2.15it/s]100%|██████████| 2/2 [00:00<00:00,  2.04it/s]
all_samples: 53
list_of_results: 53
global MRR: 0.0013986238796301611
global Precision at 10: 0.0
global Precision at 1: 0.0


P@1 : 0.0

{'description': 'location of the item, '
                'physical object or '
                'event is within. In '
                'case of an '
                'administrative entity '
                'use P131. In case of a '
                'distinct terrain '
                'feature use P706.',
 'label': 'location',
 'relation': 'P276',
 'template': '[X] is located in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P276.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is located in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P276', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
959
766
distinct template facts: 765
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:00<00:12,  1.89it/s]  8%|▊         | 2/24 [00:01<00:12,  1.83it/s] 12%|█▎        | 3/24 [00:01<00:11,  1.81it/s] 17%|█▋        | 4/24 [00:02<00:11,  1.73it/s] 21%|██        | 5/24 [00:02<00:11,  1.71it/s] 25%|██▌       | 6/24 [00:03<00:10,  1.72it/s] 29%|██▉       | 7/24 [00:03<00:09,  1.76it/s] 33%|███▎      | 8/24 [00:04<00:09,  1.75it/s] 38%|███▊      | 9/24 [00:05<00:08,  1.74it/s] 42%|████▏     | 10/24 [00:05<00:08,  1.74it/s] 46%|████▌     | 11/24 [00:06<00:07,  1.76it/s] 50%|█████     | 12/24 [00:06<00:06,  1.78it/s] 54%|█████▍    | 13/24 [00:07<00:06,  1.80it/s] 58%|█████▊    | 14/24 [00:07<00:05,  1.79it/s] 62%|██████▎   | 15/24 [00:08<00:05,  1.77it/s] 67%|██████▋   | 16/24 [00:09<00:04,  1.73it/s] 71%|███████   | 17/24 [00:09<00:04,  1.69it/s] 75%|███████▌  | 18/24 [00:10<00:03,  1.69it/s] 79%|███████▉  | 19/24 [00:10<00:02,  1.68it/s] 83%|████████▎ | 20/24 [00:11<00:02,  1.70it/s] 88%|████████▊ | 21/24 [00:12<00:01,  1.71it/s] 92%|█████████▏| 22/24 [00:12<00:01,  1.72it/s] 96%|█████████▌| 23/24 [00:13<00:00,  1.72it/s]100%|██████████| 24/24 [00:13<00:00,  1.77it/s]100%|██████████| 24/24 [00:13<00:00,  1.74it/s]
all_samples: 765
list_of_results: 765
global MRR: 0.458103268526422
global Precision at 10: 0.6235294117647059
global Precision at 1: 0.37254901960784315


P@1 : 0.37254901960784315

{'description': 'location where persons '
                'were active',
 'label': 'work location',
 'relation': 'P937',
 'template': '[X] used to work in [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P937.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] used to work in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P937', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
954
853
distinct template facts: 853
  0%|          | 0/27 [00:00<?, ?it/s]  4%|▎         | 1/27 [00:00<00:16,  1.60it/s]  7%|▋         | 2/27 [00:01<00:15,  1.63it/s] 11%|█         | 3/27 [00:01<00:14,  1.66it/s] 15%|█▍        | 4/27 [00:02<00:14,  1.63it/s] 19%|█▊        | 5/27 [00:03<00:13,  1.64it/s] 22%|██▏       | 6/27 [00:03<00:12,  1.67it/s] 26%|██▌       | 7/27 [00:04<00:11,  1.68it/s] 30%|██▉       | 8/27 [00:04<00:11,  1.69it/s] 33%|███▎      | 9/27 [00:05<00:10,  1.72it/s] 37%|███▋      | 10/27 [00:05<00:09,  1.74it/s] 41%|████      | 11/27 [00:06<00:09,  1.74it/s] 44%|████▍     | 12/27 [00:07<00:08,  1.75it/s] 48%|████▊     | 13/27 [00:07<00:07,  1.76it/s] 52%|█████▏    | 14/27 [00:08<00:07,  1.77it/s] 56%|█████▌    | 15/27 [00:08<00:06,  1.77it/s] 59%|█████▉    | 16/27 [00:09<00:06,  1.77it/s] 63%|██████▎   | 17/27 [00:09<00:05,  1.77it/s] 67%|██████▋   | 18/27 [00:10<00:05,  1.77it/s] 70%|███████   | 19/27 [00:10<00:04,  1.78it/s] 74%|███████▍  | 20/27 [00:11<00:03,  1.78it/s] 78%|███████▊  | 21/27 [00:12<00:03,  1.79it/s] 81%|████████▏ | 22/27 [00:12<00:02,  1.79it/s] 85%|████████▌ | 23/27 [00:13<00:02,  1.80it/s] 89%|████████▉ | 24/27 [00:13<00:01,  1.75it/s] 93%|█████████▎| 25/27 [00:14<00:01,  1.71it/s] 96%|█████████▋| 26/27 [00:15<00:00,  1.68it/s]100%|██████████| 27/27 [00:15<00:00,  1.85it/s]100%|██████████| 27/27 [00:15<00:00,  1.75it/s]
all_samples: 853
list_of_results: 853
global MRR: 0.3255782475880602
global Precision at 10: 0.5064478311840562
global Precision at 1: 0.21688159437280188


P@1 : 0.21688159437280188

{'description': 'religion of a person, '
                'organization or '
                'religious building, or '
                'associated with this '
                'subject',
 'label': 'religion',
 'relation': 'P140',
 'template': '[X] is affiliated with '
             'the [Y] religion .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P140.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is affiliated with the [Y] religion .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P140', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
473
434
distinct template facts: 434
  0%|          | 0/14 [00:00<?, ?it/s]  7%|▋         | 1/14 [00:00<00:07,  1.66it/s] 14%|█▍        | 2/14 [00:01<00:07,  1.66it/s] 21%|██▏       | 3/14 [00:01<00:06,  1.71it/s] 29%|██▊       | 4/14 [00:02<00:05,  1.76it/s] 36%|███▌      | 5/14 [00:02<00:05,  1.77it/s] 43%|████▎     | 6/14 [00:03<00:04,  1.77it/s] 50%|█████     | 7/14 [00:04<00:03,  1.76it/s] 57%|█████▋    | 8/14 [00:04<00:03,  1.76it/s] 64%|██████▍   | 9/14 [00:05<00:02,  1.76it/s] 71%|███████▏  | 10/14 [00:05<00:02,  1.76it/s] 79%|███████▊  | 11/14 [00:06<00:01,  1.63it/s] 86%|████████▌ | 12/14 [00:07<00:01,  1.66it/s] 93%|█████████▎| 13/14 [00:07<00:00,  1.68it/s]100%|██████████| 14/14 [00:07<00:00,  1.96it/s]100%|██████████| 14/14 [00:07<00:00,  1.77it/s]
all_samples: 434
list_of_results: 434
global MRR: 0.06043564684381883
global Precision at 10: 0.14285714285714285
global Precision at 1: 0.009216589861751152


P@1 : 0.009216589861751152

{'description': 'musical instrument '
                'that a person plays',
 'label': 'instrument',
 'relation': 'P1303',
 'template': '[X] plays [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P1303.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] plays [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P1303', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
949
513
distinct template facts: 513
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:00<00:09,  1.76it/s] 12%|█▏        | 2/17 [00:01<00:08,  1.75it/s] 18%|█▊        | 3/17 [00:01<00:08,  1.74it/s] 24%|██▎       | 4/17 [00:02<00:07,  1.74it/s] 29%|██▉       | 5/17 [00:02<00:06,  1.74it/s] 35%|███▌      | 6/17 [00:03<00:06,  1.74it/s] 41%|████      | 7/17 [00:04<00:05,  1.74it/s] 47%|████▋     | 8/17 [00:04<00:05,  1.74it/s] 53%|█████▎    | 9/17 [00:05<00:04,  1.76it/s] 59%|█████▉    | 10/17 [00:05<00:03,  1.76it/s] 65%|██████▍   | 11/17 [00:06<00:03,  1.76it/s] 71%|███████   | 12/17 [00:06<00:02,  1.77it/s] 76%|███████▋  | 13/17 [00:07<00:02,  1.78it/s] 82%|████████▏ | 14/17 [00:07<00:01,  1.79it/s] 88%|████████▊ | 15/17 [00:08<00:01,  1.77it/s] 94%|█████████▍| 16/17 [00:09<00:00,  1.77it/s]100%|██████████| 17/17 [00:09<00:00,  1.86it/s]
all_samples: 513
list_of_results: 513
global MRR: 0.270929866808041
global Precision at 10: 0.49317738791423
global Precision at 1: 0.15594541910331383


P@1 : 0.15594541910331383

{'description': 'owner of the subject',
 'label': 'owned by',
 'relation': 'P127',
 'template': '[X] is owned by [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P127.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is owned by [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P127', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
687
619
distinct template facts: 619
  0%|          | 0/20 [00:00<?, ?it/s]  5%|▌         | 1/20 [00:00<00:11,  1.72it/s] 10%|█         | 2/20 [00:01<00:10,  1.73it/s] 15%|█▌        | 3/20 [00:01<00:09,  1.78it/s] 20%|██        | 4/20 [00:02<00:08,  1.81it/s] 25%|██▌       | 5/20 [00:02<00:08,  1.74it/s] 30%|███       | 6/20 [00:03<00:07,  1.78it/s] 35%|███▌      | 7/20 [00:03<00:07,  1.77it/s] 40%|████      | 8/20 [00:04<00:07,  1.65it/s] 45%|████▌     | 9/20 [00:05<00:06,  1.69it/s] 50%|█████     | 10/20 [00:05<00:05,  1.73it/s] 55%|█████▌    | 11/20 [00:06<00:05,  1.76it/s] 60%|██████    | 12/20 [00:06<00:04,  1.78it/s] 65%|██████▌   | 13/20 [00:07<00:03,  1.80it/s] 70%|███████   | 14/20 [00:07<00:03,  1.80it/s] 75%|███████▌  | 15/20 [00:08<00:02,  1.81it/s] 80%|████████  | 16/20 [00:09<00:02,  1.83it/s] 85%|████████▌ | 17/20 [00:09<00:01,  1.83it/s] 90%|█████████ | 18/20 [00:10<00:01,  1.79it/s] 95%|█████████▌| 19/20 [00:10<00:00,  1.76it/s]100%|██████████| 20/20 [00:10<00:00,  2.19it/s]100%|██████████| 20/20 [00:10<00:00,  1.83it/s]
all_samples: 619
list_of_results: 619
global MRR: 0.39322245236149067
global Precision at 10: 0.5912762520193862
global Precision at 1: 0.27948303715670436


P@1 : 0.27948303715670436

{'description': 'language or languages '
                'a person has learned '
                'from early childhood',
 'label': 'native language',
 'relation': 'P103',
 'template': 'The native language of '
             '[X] is [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P103.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': 'The native language of [X] is [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P103', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
977
919
distinct template facts: 919
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:15,  1.84it/s]  7%|▋         | 2/29 [00:01<00:15,  1.79it/s] 10%|█         | 3/29 [00:01<00:15,  1.71it/s] 14%|█▍        | 4/29 [00:02<00:14,  1.72it/s] 17%|█▋        | 5/29 [00:02<00:13,  1.74it/s] 21%|██        | 6/29 [00:03<00:13,  1.74it/s] 24%|██▍       | 7/29 [00:03<00:12,  1.76it/s] 28%|██▊       | 8/29 [00:04<00:11,  1.76it/s] 31%|███       | 9/29 [00:05<00:11,  1.78it/s] 34%|███▍      | 10/29 [00:05<00:10,  1.79it/s] 38%|███▊      | 11/29 [00:06<00:09,  1.80it/s] 41%|████▏     | 12/29 [00:06<00:09,  1.80it/s] 45%|████▍     | 13/29 [00:07<00:08,  1.81it/s] 48%|████▊     | 14/29 [00:07<00:08,  1.81it/s] 52%|█████▏    | 15/29 [00:08<00:07,  1.76it/s] 55%|█████▌    | 16/29 [00:09<00:07,  1.72it/s] 59%|█████▊    | 17/29 [00:09<00:06,  1.74it/s] 62%|██████▏   | 18/29 [00:10<00:06,  1.73it/s] 66%|██████▌   | 19/29 [00:10<00:05,  1.76it/s] 69%|██████▉   | 20/29 [00:11<00:05,  1.78it/s] 72%|███████▏  | 21/29 [00:11<00:04,  1.78it/s] 76%|███████▌  | 22/29 [00:12<00:03,  1.79it/s] 79%|███████▉  | 23/29 [00:13<00:03,  1.61it/s] 83%|████████▎ | 24/29 [00:13<00:03,  1.66it/s] 86%|████████▌ | 25/29 [00:14<00:02,  1.71it/s] 90%|████████▉ | 26/29 [00:14<00:01,  1.73it/s] 93%|█████████▎| 27/29 [00:15<00:01,  1.75it/s] 97%|█████████▋| 28/29 [00:15<00:00,  1.76it/s]100%|██████████| 29/29 [00:16<00:00,  1.91it/s]100%|██████████| 29/29 [00:16<00:00,  1.77it/s]
all_samples: 919
list_of_results: 919
global MRR: 0.8375479881892238
global Precision at 10: 0.9684439608269858
global Precision at 1: 0.766050054406964


P@1 : 0.766050054406964

{'description': 'twin towns, sister '
                'cities, twinned '
                'municipalities and '
                'other localities that '
                'have a partnership or '
                'cooperative agreement, '
                'either legally or '
                'informally '
                'acknowledged by their '
                'governments',
 'label': 'twinned administrative body',
 'relation': 'P190',
 'template': '[X] and [Y] are twin '
             'cities .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P190.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] and [Y] are twin cities .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P190', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
995
671
distinct template facts: 670
  0%|          | 0/21 [00:00<?, ?it/s]  5%|▍         | 1/21 [00:00<00:11,  1.78it/s] 10%|▉         | 2/21 [00:01<00:10,  1.83it/s] 14%|█▍        | 3/21 [00:01<00:10,  1.77it/s] 19%|█▉        | 4/21 [00:02<00:09,  1.79it/s] 24%|██▍       | 5/21 [00:02<00:09,  1.75it/s] 29%|██▊       | 6/21 [00:03<00:08,  1.72it/s] 33%|███▎      | 7/21 [00:03<00:07,  1.76it/s] 38%|███▊      | 8/21 [00:04<00:07,  1.80it/s] 43%|████▎     | 9/21 [00:05<00:06,  1.81it/s] 48%|████▊     | 10/21 [00:05<00:06,  1.82it/s] 52%|█████▏    | 11/21 [00:06<00:05,  1.83it/s] 57%|█████▋    | 12/21 [00:06<00:04,  1.84it/s] 62%|██████▏   | 13/21 [00:07<00:04,  1.86it/s] 67%|██████▋   | 14/21 [00:07<00:03,  1.87it/s] 71%|███████▏  | 15/21 [00:08<00:03,  1.87it/s] 76%|███████▌  | 16/21 [00:08<00:02,  1.87it/s] 81%|████████  | 17/21 [00:09<00:02,  1.87it/s] 86%|████████▌ | 18/21 [00:09<00:01,  1.82it/s] 90%|█████████ | 19/21 [00:10<00:01,  1.83it/s] 95%|█████████▌| 20/21 [00:10<00:00,  1.84it/s]100%|██████████| 21/21 [00:11<00:00,  1.89it/s]100%|██████████| 21/21 [00:11<00:00,  1.83it/s]
all_samples: 670
list_of_results: 670
global MRR: 0.0811824053776557
global Precision at 10: 0.17761194029850746
global Precision at 1: 0.025373134328358207


P@1 : 0.025373134328358207

{'description': 'the item (an '
                'institution, law, '
                'public office ...) or '
                'statement belongs to '
                'or has power over or '
                'applies to the value '
                '(a territorial '
                'jurisdiction: a '
                'country, state, '
                'municipality, ...)',
 'label': 'applies to jurisdiction',
 'relation': 'P1001',
 'template': '[X] is a legal term in '
             '[Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P1001.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a legal term in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P1001', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
701
664
distinct template facts: 664
  0%|          | 0/21 [00:00<?, ?it/s]  5%|▍         | 1/21 [00:00<00:13,  1.48it/s] 10%|▉         | 2/21 [00:01<00:11,  1.64it/s] 14%|█▍        | 3/21 [00:01<00:10,  1.70it/s] 19%|█▉        | 4/21 [00:02<00:09,  1.75it/s] 24%|██▍       | 5/21 [00:02<00:09,  1.75it/s] 29%|██▊       | 6/21 [00:03<00:08,  1.72it/s] 33%|███▎      | 7/21 [00:04<00:08,  1.70it/s] 38%|███▊      | 8/21 [00:04<00:07,  1.69it/s] 43%|████▎     | 9/21 [00:05<00:06,  1.72it/s] 48%|████▊     | 10/21 [00:05<00:06,  1.73it/s] 52%|█████▏    | 11/21 [00:06<00:05,  1.75it/s] 57%|█████▋    | 12/21 [00:06<00:05,  1.76it/s] 62%|██████▏   | 13/21 [00:07<00:04,  1.76it/s] 67%|██████▋   | 14/21 [00:08<00:04,  1.73it/s] 71%|███████▏  | 15/21 [00:08<00:03,  1.71it/s] 76%|███████▌  | 16/21 [00:09<00:02,  1.73it/s] 81%|████████  | 17/21 [00:09<00:02,  1.73it/s] 86%|████████▌ | 18/21 [00:10<00:01,  1.74it/s] 90%|█████████ | 19/21 [00:11<00:01,  1.75it/s] 95%|█████████▌| 20/21 [00:11<00:00,  1.76it/s]100%|██████████| 21/21 [00:11<00:00,  1.91it/s]100%|██████████| 21/21 [00:11<00:00,  1.75it/s]
all_samples: 664
list_of_results: 664
global MRR: 0.7057504643700636
global Precision at 10: 0.9006024096385542
global Precision at 1: 0.5828313253012049


P@1 : 0.5828313253012049

{'description': 'that class of which '
                'this subject is a '
                'particular example and '
                'member (subject '
                'typically an '
                'individual member with '
                'a proper name label); '
                'different from P279; '
                'using this property as '
                'a qualifier is '
                'deprecated—use P2868 '
                'or P3831 instead',
 'label': 'instance of',
 'relation': 'P31',
 'template': '[X] is a [Y] .',
 'type': 'N-M'}
{'dataset_filename': 'data/TREx/P31.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is a [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P31', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
922
882
distinct template facts: 882
  0%|          | 0/28 [00:00<?, ?it/s]  4%|▎         | 1/28 [00:00<00:16,  1.68it/s]  7%|▋         | 2/28 [00:01<00:15,  1.68it/s] 11%|█         | 3/28 [00:01<00:14,  1.74it/s] 14%|█▍        | 4/28 [00:02<00:13,  1.75it/s] 18%|█▊        | 5/28 [00:02<00:13,  1.75it/s] 21%|██▏       | 6/28 [00:03<00:12,  1.72it/s] 25%|██▌       | 7/28 [00:04<00:12,  1.66it/s] 29%|██▊       | 8/28 [00:04<00:12,  1.65it/s] 32%|███▏      | 9/28 [00:05<00:11,  1.66it/s] 36%|███▌      | 10/28 [00:05<00:10,  1.68it/s] 39%|███▉      | 11/28 [00:06<00:10,  1.68it/s] 43%|████▎     | 12/28 [00:07<00:09,  1.70it/s] 46%|████▋     | 13/28 [00:07<00:08,  1.72it/s] 50%|█████     | 14/28 [00:08<00:08,  1.62it/s] 54%|█████▎    | 15/28 [00:08<00:07,  1.67it/s] 57%|█████▋    | 16/28 [00:09<00:07,  1.71it/s] 61%|██████    | 17/28 [00:10<00:06,  1.74it/s] 64%|██████▍   | 18/28 [00:10<00:05,  1.76it/s] 68%|██████▊   | 19/28 [00:11<00:05,  1.77it/s] 71%|███████▏  | 20/28 [00:11<00:04,  1.78it/s] 75%|███████▌  | 21/28 [00:12<00:03,  1.79it/s] 79%|███████▊  | 22/28 [00:12<00:03,  1.78it/s] 82%|████████▏ | 23/28 [00:13<00:02,  1.78it/s] 86%|████████▌ | 24/28 [00:13<00:02,  1.77it/s] 89%|████████▉ | 25/28 [00:14<00:01,  1.76it/s] 93%|█████████▎| 26/28 [00:15<00:01,  1.74it/s] 96%|█████████▋| 27/28 [00:15<00:00,  1.75it/s]100%|██████████| 28/28 [00:15<00:00,  2.01it/s]100%|██████████| 28/28 [00:15<00:00,  1.75it/s]
all_samples: 882
list_of_results: 882
global MRR: 0.24088431219448642
global Precision at 10: 0.30385487528344673
global Precision at 1: 0.20634920634920634


P@1 : 0.20634920634920634

{'description': 'country of origin of '
                'this item (creative '
                'work, food, phrase, '
                'product, etc.)',
 'label': 'country of origin',
 'relation': 'P495',
 'template': '[X] was created in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P495.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was created in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P495', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
909
905
distinct template facts: 905
  0%|          | 0/29 [00:00<?, ?it/s]  3%|▎         | 1/29 [00:00<00:15,  1.81it/s]  7%|▋         | 2/29 [00:01<00:15,  1.78it/s] 10%|█         | 3/29 [00:01<00:14,  1.77it/s] 14%|█▍        | 4/29 [00:02<00:14,  1.72it/s] 17%|█▋        | 5/29 [00:02<00:14,  1.69it/s] 21%|██        | 6/29 [00:03<00:13,  1.68it/s] 24%|██▍       | 7/29 [00:04<00:13,  1.68it/s] 28%|██▊       | 8/29 [00:04<00:12,  1.68it/s] 31%|███       | 9/29 [00:05<00:11,  1.69it/s] 34%|███▍      | 10/29 [00:05<00:11,  1.69it/s] 38%|███▊      | 11/29 [00:06<00:10,  1.72it/s] 41%|████▏     | 12/29 [00:06<00:09,  1.75it/s] 45%|████▍     | 13/29 [00:07<00:09,  1.74it/s] 48%|████▊     | 14/29 [00:08<00:08,  1.76it/s] 52%|█████▏    | 15/29 [00:08<00:07,  1.79it/s] 55%|█████▌    | 16/29 [00:09<00:07,  1.81it/s] 59%|█████▊    | 17/29 [00:09<00:06,  1.82it/s] 62%|██████▏   | 18/29 [00:10<00:06,  1.83it/s] 66%|██████▌   | 19/29 [00:10<00:05,  1.84it/s] 69%|██████▉   | 20/29 [00:11<00:04,  1.83it/s] 72%|███████▏  | 21/29 [00:12<00:04,  1.66it/s] 76%|███████▌  | 22/29 [00:12<00:04,  1.69it/s] 79%|███████▉  | 23/29 [00:13<00:03,  1.63it/s] 83%|████████▎ | 24/29 [00:13<00:03,  1.65it/s] 86%|████████▌ | 25/29 [00:14<00:02,  1.67it/s] 90%|████████▉ | 26/29 [00:15<00:01,  1.69it/s] 93%|█████████▎| 27/29 [00:15<00:01,  1.66it/s] 97%|█████████▋| 28/29 [00:16<00:00,  1.67it/s]100%|██████████| 29/29 [00:16<00:00,  2.14it/s]100%|██████████| 29/29 [00:16<00:00,  1.76it/s]
all_samples: 905
list_of_results: 905
global MRR: 0.027273023285924364
global Precision at 10: 0.026519337016574586
global Precision at 1: 0.013259668508287293


P@1 : 0.013259668508287293

{'description': 'specific location '
                'where an '
                "organization's "
                'headquarters is or has '
                'been situated. Inverse '
                'property of "occupant" '
                '(P466).',
 'label': 'headquarters location',
 'relation': 'P159',
 'template': 'The headquarter of [X] is '
             'in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P159.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': 'The headquarter of [X] is in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P159', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
967
801
distinct template facts: 801
  0%|          | 0/26 [00:00<?, ?it/s]  4%|▍         | 1/26 [00:00<00:16,  1.49it/s]  8%|▊         | 2/26 [00:01<00:14,  1.60it/s] 12%|█▏        | 3/26 [00:01<00:13,  1.65it/s] 15%|█▌        | 4/26 [00:02<00:13,  1.68it/s] 19%|█▉        | 5/26 [00:03<00:12,  1.69it/s] 23%|██▎       | 6/26 [00:03<00:11,  1.70it/s] 27%|██▋       | 7/26 [00:04<00:11,  1.69it/s] 31%|███       | 8/26 [00:04<00:10,  1.69it/s] 35%|███▍      | 9/26 [00:05<00:10,  1.68it/s] 38%|███▊      | 10/26 [00:05<00:09,  1.70it/s] 42%|████▏     | 11/26 [00:06<00:08,  1.72it/s] 46%|████▌     | 12/26 [00:07<00:08,  1.66it/s] 50%|█████     | 13/26 [00:07<00:07,  1.66it/s] 54%|█████▍    | 14/26 [00:08<00:07,  1.67it/s] 58%|█████▊    | 15/26 [00:08<00:06,  1.66it/s] 62%|██████▏   | 16/26 [00:09<00:05,  1.68it/s] 65%|██████▌   | 17/26 [00:10<00:05,  1.67it/s] 69%|██████▉   | 18/26 [00:10<00:04,  1.69it/s] 73%|███████▎  | 19/26 [00:11<00:04,  1.70it/s] 77%|███████▋  | 20/26 [00:11<00:03,  1.70it/s] 81%|████████  | 21/26 [00:12<00:02,  1.71it/s] 85%|████████▍ | 22/26 [00:13<00:02,  1.72it/s] 88%|████████▊ | 23/26 [00:13<00:01,  1.70it/s] 92%|█████████▏| 24/26 [00:14<00:01,  1.66it/s] 96%|█████████▌| 25/26 [00:14<00:00,  1.62it/s]100%|██████████| 26/26 [00:14<00:00,  1.74it/s]
all_samples: 801
list_of_results: 801
global MRR: 0.4587541591364881
global Precision at 10: 0.6466916354556804
global Precision at 1: 0.3620474406991261


P@1 : 0.3620474406991261

{'description': 'primary city of a '
                'country, state or '
                'other type of '
                'administrative '
                'territorial entity',
 'label': 'capital',
 'relation': 'P36',
 'template': 'The capital of [X] is [Y] '
             '.',
 'type': '1-1'}
{'dataset_filename': 'data/TREx/P36.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': 'The capital of [X] is [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P36', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
703
471
distinct template facts: 470
  0%|          | 0/15 [00:00<?, ?it/s]  7%|▋         | 1/15 [00:00<00:08,  1.61it/s] 13%|█▎        | 2/15 [00:01<00:07,  1.71it/s] 20%|██        | 3/15 [00:01<00:06,  1.76it/s] 27%|██▋       | 4/15 [00:02<00:06,  1.76it/s] 33%|███▎      | 5/15 [00:02<00:05,  1.76it/s] 40%|████      | 6/15 [00:03<00:05,  1.78it/s] 47%|████▋     | 7/15 [00:03<00:04,  1.77it/s] 53%|█████▎    | 8/15 [00:04<00:03,  1.78it/s] 60%|██████    | 9/15 [00:05<00:03,  1.79it/s] 67%|██████▋   | 10/15 [00:05<00:02,  1.78it/s] 73%|███████▎  | 11/15 [00:06<00:02,  1.79it/s] 80%|████████  | 12/15 [00:06<00:01,  1.79it/s] 87%|████████▋ | 13/15 [00:07<00:01,  1.73it/s] 93%|█████████▎| 14/15 [00:07<00:00,  1.72it/s]100%|██████████| 15/15 [00:08<00:00,  1.90it/s]100%|██████████| 15/15 [00:08<00:00,  1.79it/s]
all_samples: 470
list_of_results: 470
global MRR: 0.6065074394621409
global Precision at 10: 0.7446808510638298
global Precision at 1: 0.5212765957446809


P@1 : 0.5212765957446809

{'description': 'location where a group '
                'or organization was '
                'formed',
 'label': 'location of formation',
 'relation': 'P740',
 'template': '[X] was founded in [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P740.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] was founded in [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P740', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
936
843
distinct template facts: 843
  0%|          | 0/27 [00:00<?, ?it/s]  4%|▎         | 1/27 [00:00<00:14,  1.77it/s]  7%|▋         | 2/27 [00:01<00:14,  1.69it/s] 11%|█         | 3/27 [00:01<00:14,  1.69it/s] 15%|█▍        | 4/27 [00:02<00:13,  1.69it/s] 19%|█▊        | 5/27 [00:02<00:12,  1.69it/s] 22%|██▏       | 6/27 [00:03<00:12,  1.69it/s] 26%|██▌       | 7/27 [00:04<00:11,  1.72it/s] 30%|██▉       | 8/27 [00:04<00:10,  1.76it/s] 33%|███▎      | 9/27 [00:05<00:10,  1.78it/s] 37%|███▋      | 10/27 [00:05<00:09,  1.80it/s] 41%|████      | 11/27 [00:06<00:08,  1.80it/s] 44%|████▍     | 12/27 [00:06<00:08,  1.67it/s] 48%|████▊     | 13/27 [00:07<00:08,  1.72it/s] 52%|█████▏    | 14/27 [00:08<00:07,  1.74it/s] 56%|█████▌    | 15/27 [00:08<00:06,  1.76it/s] 59%|█████▉    | 16/27 [00:09<00:06,  1.78it/s] 63%|██████▎   | 17/27 [00:09<00:05,  1.78it/s] 67%|██████▋   | 18/27 [00:10<00:05,  1.79it/s] 70%|███████   | 19/27 [00:10<00:04,  1.80it/s] 74%|███████▍  | 20/27 [00:11<00:03,  1.81it/s] 78%|███████▊  | 21/27 [00:11<00:03,  1.80it/s] 81%|████████▏ | 22/27 [00:12<00:02,  1.80it/s] 85%|████████▌ | 23/27 [00:13<00:02,  1.80it/s] 89%|████████▉ | 24/27 [00:13<00:01,  1.81it/s] 93%|█████████▎| 25/27 [00:14<00:01,  1.82it/s] 96%|█████████▋| 26/27 [00:14<00:00,  1.79it/s]100%|██████████| 27/27 [00:14<00:00,  2.19it/s]100%|██████████| 27/27 [00:14<00:00,  1.81it/s]
all_samples: 843
list_of_results: 843
global MRR: 0.00880520594407697
global Precision at 10: 0.002372479240806643
global Precision at 1: 0.002372479240806643


P@1 : 0.002372479240806643

{'description': 'object of which the '
                'subject is a part '
                "(it's not useful to "
                'link objects which are '
                'themselves parts of '
                'other objects already '
                'listed as parts of the '
                'subject). Inverse '
                'property of "has part" '
                '(P527, see also "has '
                'parts of the class" '
                '(P2670)).',
 'label': 'part of',
 'relation': 'P361',
 'template': '[X] is part of [Y] .',
 'type': 'N-1'}
{'dataset_filename': 'data/TREx/P361.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '[X] is part of [Y] .', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/P361', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
<lama.modules.roberta_connector.Roberta object at 0x7f291ca1d390>
932
756
distinct template facts: 756
  0%|          | 0/24 [00:00<?, ?it/s]  4%|▍         | 1/24 [00:00<00:13,  1.71it/s]  8%|▊         | 2/24 [00:01<00:12,  1.73it/s] 12%|█▎        | 3/24 [00:01<00:12,  1.69it/s] 17%|█▋        | 4/24 [00:02<00:12,  1.66it/s] 21%|██        | 5/24 [00:02<00:11,  1.66it/s] 25%|██▌       | 6/24 [00:03<00:10,  1.66it/s] 29%|██▉       | 7/24 [00:04<00:10,  1.68it/s] 33%|███▎      | 8/24 [00:04<00:09,  1.68it/s] 38%|███▊      | 9/24 [00:05<00:08,  1.70it/s] 42%|████▏     | 10/24 [00:05<00:08,  1.69it/s] 46%|████▌     | 11/24 [00:06<00:07,  1.73it/s] 50%|█████     | 12/24 [00:07<00:06,  1.75it/s] 54%|█████▍    | 13/24 [00:07<00:06,  1.77it/s] 58%|█████▊    | 14/24 [00:08<00:05,  1.77it/s] 62%|██████▎   | 15/24 [00:08<00:05,  1.78it/s] 67%|██████▋   | 16/24 [00:09<00:04,  1.78it/s] 71%|███████   | 17/24 [00:09<00:03,  1.78it/s] 75%|███████▌  | 18/24 [00:10<00:03,  1.78it/s] 79%|███████▉  | 19/24 [00:10<00:02,  1.78it/s] 83%|████████▎ | 20/24 [00:11<00:02,  1.77it/s] 88%|████████▊ | 21/24 [00:12<00:01,  1.62it/s] 92%|█████████▏| 22/24 [00:12<00:01,  1.67it/s] 96%|█████████▌| 23/24 [00:13<00:00,  1.69it/s]100%|██████████| 24/24 [00:13<00:00,  1.88it/s]100%|██████████| 24/24 [00:13<00:00,  1.74it/s]
all_samples: 756
list_of_results: 756
global MRR: 0.23817017233494492
global Precision at 10: 0.3835978835978836
global Precision at 1: 0.16137566137566137


P@1 : 0.16137566137566137
@@@ roberta_base_mp0.2 - mean P@1: 0.25058892925651377
@@@  roberta_base_mp0.2 N-1 0.2461639363842222 20006 23
@@@  roberta_base_mp0.2 N-M 0.21972999828261663 13096 16
@@@  roberta_base_mp0.2 1-1 0.5483477950790443 937 2

2022-08-18 17:27:17 | INFO | fairseq.file_utils | loading archive file pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128
2022-08-18 17:27:18 | INFO | fairseq.tasks.masked_lm | dictionary: 51199 types
2022-08-18 17:27:21 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair1212:17242', 'distributed_port': 17242, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': 50000, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_base', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=True, continue_once=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=17242, distributed_rank=0, distributed_world_size=128, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=True, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, include_target_tokens=False, keep_best_checkpoints=-1, keep_interval_updates=1, keep_interval_updates_pattern=50000, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0006], lr_scheduler='polynomial_decay', mask_impl='fix', mask_multiple_length=1, mask_prob=0.2, mask_prob_range='none', mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_positions=512, max_source_positions=512, max_tokens=None, max_tokens_valid=None, max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, seqlen_masking_boundary='none', seqlen_masking_flag=False, seqlen_masking_granularity='none', shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='masked_lm', tensorboard_logdir='/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='500000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_updates=24000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'masked_lm', 'data': '/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.2, 'mask_prob_range': 'none', 'mask_impl': 'fix', 'seqlen_masking_flag': False, 'seqlen_masking_granularity': 'none', 'seqlen_masking_boundary': 'none', 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': True, 'tpu': False, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
3. ConceptNet
roberta_base_mp0.2
{'relation': 'test'}
{'dataset_filename': 'data/ConceptNet/test.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/test', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Loading roberta model...
<lama.modules.roberta_connector.Roberta object at 0x7f26410c7390>
29774
12262
  0%|          | 0/384 [00:00<?, ?it/s]  0%|          | 1/384 [00:00<04:17,  1.49it/s]  1%|          | 2/384 [00:01<03:47,  1.68it/s]  1%|          | 3/384 [00:01<03:41,  1.72it/s]  1%|          | 4/384 [00:02<03:35,  1.76it/s]  1%|▏         | 5/384 [00:02<03:31,  1.80it/s]  2%|▏         | 6/384 [00:03<03:29,  1.80it/s]  2%|▏         | 7/384 [00:03<03:28,  1.81it/s]  2%|▏         | 8/384 [00:04<03:30,  1.78it/s]  2%|▏         | 9/384 [00:05<03:32,  1.76it/s]  3%|▎         | 10/384 [00:05<03:29,  1.79it/s]  3%|▎         | 11/384 [00:06<03:26,  1.81it/s]  3%|▎         | 12/384 [00:06<03:23,  1.82it/s]  3%|▎         | 13/384 [00:07<03:22,  1.83it/s]  4%|▎         | 14/384 [00:07<03:33,  1.73it/s]  4%|▍         | 15/384 [00:08<03:38,  1.69it/s]  4%|▍         | 16/384 [00:09<03:41,  1.66it/s]  4%|▍         | 17/384 [00:09<03:40,  1.66it/s]  5%|▍         | 18/384 [00:10<03:40,  1.66it/s]  5%|▍         | 19/384 [00:10<03:36,  1.69it/s]  5%|▌         | 20/384 [00:11<03:33,  1.70it/s]  5%|▌         | 21/384 [00:12<03:56,  1.53it/s]  6%|▌         | 22/384 [00:12<03:46,  1.60it/s]  6%|▌         | 23/384 [00:13<03:37,  1.66it/s]  6%|▋         | 24/384 [00:14<03:32,  1.69it/s]  7%|▋         | 25/384 [00:14<03:34,  1.67it/s]  7%|▋         | 26/384 [00:15<03:33,  1.67it/s]  7%|▋         | 27/384 [00:15<03:28,  1.71it/s]  7%|▋         | 28/384 [00:16<03:23,  1.75it/s]  8%|▊         | 29/384 [00:16<03:20,  1.77it/s]  8%|▊         | 30/384 [00:17<03:17,  1.79it/s]  8%|▊         | 31/384 [00:17<03:17,  1.79it/s]  8%|▊         | 32/384 [00:18<03:17,  1.78it/s]  9%|▊         | 33/384 [00:19<03:14,  1.80it/s]  9%|▉         | 34/384 [00:19<03:12,  1.82it/s]  9%|▉         | 35/384 [00:20<03:10,  1.83it/s]  9%|▉         | 36/384 [00:20<03:09,  1.84it/s] 10%|▉         | 37/384 [00:21<03:08,  1.84it/s] 10%|▉         | 38/384 [00:21<03:06,  1.85it/s] 10%|█         | 39/384 [00:22<03:06,  1.85it/s] 10%|█         | 40/384 [00:22<03:05,  1.86it/s] 11%|█         | 41/384 [00:23<03:05,  1.85it/s] 11%|█         | 42/384 [00:23<03:04,  1.86it/s] 11%|█         | 43/384 [00:24<03:03,  1.86it/s] 11%|█▏        | 44/384 [00:25<03:03,  1.85it/s] 12%|█▏        | 45/384 [00:25<03:03,  1.84it/s] 12%|█▏        | 46/384 [00:26<03:06,  1.81it/s] 12%|█▏        | 47/384 [00:26<03:05,  1.82it/s] 12%|█▎        | 48/384 [00:27<03:03,  1.83it/s] 13%|█▎        | 49/384 [00:27<03:03,  1.83it/s] 13%|█▎        | 50/384 [00:28<03:02,  1.83it/s] 13%|█▎        | 51/384 [00:28<03:01,  1.83it/s] 14%|█▎        | 52/384 [00:29<03:01,  1.83it/s] 14%|█▍        | 53/384 [00:29<03:01,  1.82it/s] 14%|█▍        | 54/384 [00:30<03:01,  1.82it/s] 14%|█▍        | 55/384 [00:31<03:01,  1.82it/s] 15%|█▍        | 56/384 [00:31<02:59,  1.83it/s] 15%|█▍        | 57/384 [00:32<02:57,  1.84it/s] 15%|█▌        | 58/384 [00:32<02:57,  1.84it/s] 15%|█▌        | 59/384 [00:33<02:56,  1.84it/s] 16%|█▌        | 60/384 [00:33<02:55,  1.85it/s] 16%|█▌        | 61/384 [00:34<02:58,  1.81it/s] 16%|█▌        | 62/384 [00:34<03:00,  1.79it/s] 16%|█▋        | 63/384 [00:35<02:57,  1.80it/s] 17%|█▋        | 64/384 [00:35<02:56,  1.82it/s] 17%|█▋        | 65/384 [00:36<02:54,  1.82it/s] 17%|█▋        | 66/384 [00:37<02:53,  1.83it/s] 17%|█▋        | 67/384 [00:37<02:52,  1.83it/s] 18%|█▊        | 68/384 [00:38<02:52,  1.83it/s] 18%|█▊        | 69/384 [00:38<02:51,  1.84it/s] 18%|█▊        | 70/384 [00:39<02:50,  1.84it/s] 18%|█▊        | 71/384 [00:39<02:58,  1.75it/s] 19%|█▉        | 72/384 [00:40<02:59,  1.74it/s] 19%|█▉        | 73/384 [00:41<02:56,  1.76it/s] 19%|█▉        | 74/384 [00:41<02:54,  1.78it/s] 20%|█▉        | 75/384 [00:42<02:52,  1.79it/s] 20%|█▉        | 76/384 [00:42<02:57,  1.74it/s] 20%|██        | 77/384 [00:43<02:53,  1.76it/s] 20%|██        | 78/384 [00:43<02:50,  1.80it/s] 21%|██        | 79/384 [00:44<02:47,  1.82it/s] 21%|██        | 80/384 [00:44<02:45,  1.84it/s] 21%|██        | 81/384 [00:45<02:44,  1.85it/s] 21%|██▏       | 82/384 [00:45<02:43,  1.85it/s] 22%|██▏       | 83/384 [00:46<02:42,  1.85it/s] 22%|██▏       | 84/384 [00:47<02:48,  1.78it/s] 22%|██▏       | 85/384 [00:47<02:51,  1.74it/s] 22%|██▏       | 86/384 [00:48<02:52,  1.72it/s] 23%|██▎       | 87/384 [00:48<02:48,  1.76it/s] 23%|██▎       | 88/384 [00:49<02:46,  1.78it/s] 23%|██▎       | 89/384 [00:49<02:44,  1.79it/s] 23%|██▎       | 90/384 [00:50<02:42,  1.81it/s] 24%|██▎       | 91/384 [00:51<02:44,  1.78it/s] 24%|██▍       | 92/384 [00:51<02:43,  1.79it/s] 24%|██▍       | 93/384 [00:52<02:40,  1.81it/s] 24%|██▍       | 94/384 [00:52<02:39,  1.82it/s] 25%|██▍       | 95/384 [00:53<02:38,  1.83it/s] 25%|██▌       | 96/384 [00:53<02:38,  1.82it/s] 25%|██▌       | 97/384 [00:54<02:37,  1.82it/s] 26%|██▌       | 98/384 [00:54<02:36,  1.82it/s] 26%|██▌       | 99/384 [00:55<02:35,  1.83it/s] 26%|██▌       | 100/384 [00:56<03:49,  1.24it/s] 26%|██▋       | 101/384 [00:57<03:26,  1.37it/s] 27%|██▋       | 102/384 [00:57<03:11,  1.47it/s] 27%|██▋       | 103/384 [00:58<03:00,  1.56it/s] 27%|██▋       | 104/384 [00:59<02:56,  1.59it/s] 27%|██▋       | 105/384 [00:59<02:48,  1.66it/s] 28%|██▊       | 106/384 [01:00<02:42,  1.71it/s] 28%|██▊       | 107/384 [01:00<02:38,  1.74it/s] 28%|██▊       | 108/384 [01:01<02:43,  1.68it/s] 28%|██▊       | 109/384 [01:01<02:40,  1.72it/s] 29%|██▊       | 110/384 [01:02<02:38,  1.73it/s] 29%|██▉       | 111/384 [01:03<02:36,  1.74it/s] 29%|██▉       | 112/384 [01:03<02:35,  1.75it/s] 29%|██▉       | 113/384 [01:04<02:33,  1.77it/s] 30%|██▉       | 114/384 [01:04<02:31,  1.78it/s] 30%|██▉       | 115/384 [01:05<02:30,  1.79it/s] 30%|███       | 116/384 [01:05<02:29,  1.79it/s] 30%|███       | 117/384 [01:06<02:27,  1.81it/s] 31%|███       | 118/384 [01:06<02:27,  1.80it/s] 31%|███       | 119/384 [01:07<02:29,  1.77it/s] 31%|███▏      | 120/384 [01:08<02:33,  1.73it/s] 32%|███▏      | 121/384 [01:08<02:33,  1.71it/s] 32%|███▏      | 122/384 [01:09<02:30,  1.74it/s] 32%|███▏      | 123/384 [01:09<02:27,  1.77it/s] 32%|███▏      | 124/384 [01:10<02:24,  1.79it/s] 33%|███▎      | 125/384 [01:10<02:23,  1.81it/s] 33%|███▎      | 126/384 [01:11<02:21,  1.82it/s] 33%|███▎      | 127/384 [01:11<02:21,  1.82it/s] 33%|███▎      | 128/384 [01:12<02:19,  1.83it/s] 34%|███▎      | 129/384 [01:13<02:19,  1.83it/s] 34%|███▍      | 130/384 [01:13<02:18,  1.83it/s] 34%|███▍      | 131/384 [01:14<02:18,  1.83it/s] 34%|███▍      | 132/384 [01:14<02:18,  1.82it/s] 35%|███▍      | 133/384 [01:15<02:18,  1.81it/s] 35%|███▍      | 134/384 [01:15<02:18,  1.81it/s] 35%|███▌      | 135/384 [01:16<02:18,  1.80it/s] 35%|███▌      | 136/384 [01:16<02:17,  1.81it/s] 36%|███▌      | 137/384 [01:17<02:16,  1.81it/s] 36%|███▌      | 138/384 [01:18<02:15,  1.82it/s] 36%|███▌      | 139/384 [01:18<02:14,  1.82it/s] 36%|███▋      | 140/384 [01:19<02:13,  1.82it/s] 37%|███▋      | 141/384 [01:19<02:13,  1.83it/s] 37%|███▋      | 142/384 [01:20<02:13,  1.81it/s] 37%|███▋      | 143/384 [01:20<02:13,  1.81it/s] 38%|███▊      | 144/384 [01:21<02:12,  1.82it/s] 38%|███▊      | 145/384 [01:21<02:12,  1.80it/s] 38%|███▊      | 146/384 [01:22<02:12,  1.80it/s] 38%|███▊      | 147/384 [01:23<02:12,  1.79it/s] 39%|███▊      | 148/384 [01:23<02:14,  1.76it/s] 39%|███▉      | 149/384 [01:24<02:12,  1.78it/s] 39%|███▉      | 150/384 [01:24<02:10,  1.79it/s] 39%|███▉      | 151/384 [01:25<02:09,  1.80it/s] 40%|███▉      | 152/384 [01:25<02:08,  1.81it/s] 40%|███▉      | 153/384 [01:26<02:07,  1.82it/s] 40%|████      | 154/384 [01:26<02:07,  1.81it/s] 40%|████      | 155/384 [01:27<02:06,  1.81it/s] 41%|████      | 156/384 [01:28<02:07,  1.79it/s] 41%|████      | 157/384 [01:28<02:06,  1.80it/s] 41%|████      | 158/384 [01:29<02:05,  1.81it/s] 41%|████▏     | 159/384 [01:29<02:04,  1.80it/s] 42%|████▏     | 160/384 [01:30<02:06,  1.77it/s] 42%|████▏     | 161/384 [01:30<02:06,  1.76it/s] 42%|████▏     | 162/384 [01:31<02:09,  1.72it/s] 42%|████▏     | 163/384 [01:32<02:10,  1.70it/s] 43%|████▎     | 164/384 [01:32<02:09,  1.70it/s] 43%|████▎     | 165/384 [01:33<02:07,  1.72it/s] 43%|████▎     | 166/384 [01:33<02:06,  1.72it/s] 43%|████▎     | 167/384 [01:34<02:05,  1.73it/s] 44%|████▍     | 168/384 [01:34<02:05,  1.72it/s] 44%|████▍     | 169/384 [01:35<02:05,  1.71it/s] 44%|████▍     | 170/384 [01:36<02:03,  1.73it/s] 45%|████▍     | 171/384 [01:36<02:03,  1.72it/s] 45%|████▍     | 172/384 [01:37<02:03,  1.72it/s] 45%|████▌     | 173/384 [01:37<02:02,  1.72it/s] 45%|████▌     | 174/384 [01:38<02:01,  1.73it/s] 46%|████▌     | 175/384 [01:39<02:00,  1.73it/s] 46%|████▌     | 176/384 [01:39<02:00,  1.72it/s] 46%|████▌     | 177/384 [01:40<02:01,  1.70it/s] 46%|████▋     | 178/384 [01:40<02:01,  1.70it/s] 47%|████▋     | 179/384 [01:41<02:00,  1.71it/s] 47%|████▋     | 180/384 [01:41<01:59,  1.71it/s] 47%|████▋     | 181/384 [01:42<01:58,  1.71it/s] 47%|████▋     | 182/384 [01:43<01:58,  1.71it/s] 48%|████▊     | 183/384 [01:45<03:23,  1.01s/it] 48%|████▊     | 184/384 [01:45<02:56,  1.13it/s] 48%|████▊     | 185/384 [01:46<02:37,  1.26it/s] 48%|████▊     | 186/384 [01:46<02:22,  1.38it/s] 49%|████▊     | 187/384 [01:47<02:13,  1.48it/s] 49%|████▉     | 188/384 [01:48<02:07,  1.54it/s] 49%|████▉     | 189/384 [01:48<02:02,  1.60it/s] 49%|████▉     | 190/384 [01:49<01:59,  1.62it/s] 50%|████▉     | 191/384 [01:49<01:56,  1.66it/s] 50%|█████     | 192/384 [01:50<01:53,  1.70it/s] 50%|█████     | 193/384 [01:50<01:51,  1.72it/s] 51%|█████     | 194/384 [01:51<01:49,  1.74it/s] 51%|█████     | 195/384 [01:52<01:47,  1.75it/s] 51%|█████     | 196/384 [01:52<01:46,  1.77it/s] 51%|█████▏    | 197/384 [01:53<01:45,  1.78it/s] 52%|█████▏    | 198/384 [01:53<01:43,  1.79it/s] 52%|█████▏    | 199/384 [01:54<01:42,  1.80it/s] 52%|█████▏    | 200/384 [01:54<01:42,  1.80it/s] 52%|█████▏    | 201/384 [01:55<01:41,  1.80it/s] 53%|█████▎    | 202/384 [01:55<01:42,  1.78it/s] 53%|█████▎    | 203/384 [01:56<01:43,  1.76it/s] 53%|█████▎    | 204/384 [01:57<01:41,  1.77it/s] 53%|█████▎    | 205/384 [01:57<01:40,  1.77it/s] 54%|█████▎    | 206/384 [01:58<01:39,  1.79it/s] 54%|█████▍    | 207/384 [01:58<01:38,  1.80it/s] 54%|█████▍    | 208/384 [01:59<01:37,  1.81it/s] 54%|█████▍    | 209/384 [01:59<01:36,  1.81it/s] 55%|█████▍    | 210/384 [02:00<01:35,  1.82it/s] 55%|█████▍    | 211/384 [02:00<01:34,  1.82it/s] 55%|█████▌    | 212/384 [02:01<01:34,  1.81it/s] 55%|█████▌    | 213/384 [02:02<01:34,  1.82it/s] 56%|█████▌    | 214/384 [02:02<01:34,  1.80it/s] 56%|█████▌    | 215/384 [02:03<01:33,  1.80it/s] 56%|█████▋    | 216/384 [02:03<01:33,  1.81it/s] 57%|█████▋    | 217/384 [02:04<01:34,  1.77it/s] 57%|█████▋    | 218/384 [02:04<01:33,  1.77it/s] 57%|█████▋    | 219/384 [02:05<01:32,  1.78it/s] 57%|█████▋    | 220/384 [02:05<01:32,  1.78it/s] 58%|█████▊    | 221/384 [02:06<01:31,  1.78it/s] 58%|█████▊    | 222/384 [02:07<01:31,  1.77it/s] 58%|█████▊    | 223/384 [02:07<01:30,  1.78it/s] 58%|█████▊    | 224/384 [02:08<01:30,  1.77it/s] 59%|█████▊    | 225/384 [02:08<01:29,  1.78it/s] 59%|█████▉    | 226/384 [02:09<01:28,  1.78it/s] 59%|█████▉    | 227/384 [02:09<01:28,  1.76it/s] 59%|█████▉    | 228/384 [02:10<01:28,  1.76it/s] 60%|█████▉    | 229/384 [02:11<01:28,  1.76it/s] 60%|█████▉    | 230/384 [02:11<01:27,  1.76it/s] 60%|██████    | 231/384 [02:12<01:26,  1.77it/s] 60%|██████    | 232/384 [02:12<01:28,  1.71it/s] 61%|██████    | 233/384 [02:13<01:27,  1.73it/s] 61%|██████    | 234/384 [02:13<01:26,  1.74it/s] 61%|██████    | 235/384 [02:14<01:24,  1.75it/s] 61%|██████▏   | 236/384 [02:15<01:23,  1.76it/s] 62%|██████▏   | 237/384 [02:15<01:23,  1.77it/s] 62%|██████▏   | 238/384 [02:16<01:22,  1.77it/s] 62%|██████▏   | 239/384 [02:16<01:21,  1.77it/s] 62%|██████▎   | 240/384 [02:17<01:20,  1.78it/s] 63%|██████▎   | 241/384 [02:17<01:20,  1.78it/s] 63%|██████▎   | 242/384 [02:18<01:19,  1.79it/s] 63%|██████▎   | 243/384 [02:18<01:18,  1.79it/s] 64%|██████▎   | 244/384 [02:19<01:18,  1.78it/s] 64%|██████▍   | 245/384 [02:20<01:18,  1.76it/s] 64%|██████▍   | 246/384 [02:20<01:20,  1.72it/s] 64%|██████▍   | 247/384 [02:21<01:18,  1.74it/s] 65%|██████▍   | 248/384 [02:21<01:17,  1.76it/s] 65%|██████▍   | 249/384 [02:22<01:15,  1.78it/s] 65%|██████▌   | 250/384 [02:22<01:15,  1.78it/s] 65%|██████▌   | 251/384 [02:23<01:14,  1.79it/s] 66%|██████▌   | 252/384 [02:24<01:13,  1.80it/s] 66%|██████▌   | 253/384 [02:24<01:13,  1.79it/s] 66%|██████▌   | 254/384 [02:25<01:12,  1.80it/s] 66%|██████▋   | 255/384 [02:25<01:11,  1.80it/s] 67%|██████▋   | 256/384 [02:26<01:11,  1.78it/s] 67%|██████▋   | 257/384 [02:26<01:13,  1.73it/s] 67%|██████▋   | 258/384 [02:27<01:12,  1.74it/s] 67%|██████▋   | 259/384 [02:28<01:11,  1.76it/s] 68%|██████▊   | 260/384 [02:28<01:10,  1.75it/s] 68%|██████▊   | 261/384 [02:29<01:10,  1.73it/s] 68%|██████▊   | 262/384 [02:29<01:09,  1.75it/s] 68%|██████▊   | 263/384 [02:30<01:09,  1.75it/s] 69%|██████▉   | 264/384 [02:30<01:08,  1.75it/s] 69%|██████▉   | 265/384 [02:31<01:07,  1.76it/s] 69%|██████▉   | 266/384 [02:32<01:07,  1.76it/s] 70%|██████▉   | 267/384 [02:32<01:06,  1.77it/s] 70%|██████▉   | 268/384 [02:33<01:05,  1.77it/s] 70%|███████   | 269/384 [02:33<01:04,  1.77it/s] 70%|███████   | 270/384 [02:34<01:04,  1.77it/s] 71%|███████   | 271/384 [02:37<02:20,  1.25s/it] 71%|███████   | 272/384 [02:37<01:56,  1.04s/it] 71%|███████   | 273/384 [02:38<01:39,  1.11it/s] 71%|███████▏  | 274/384 [02:38<01:27,  1.26it/s] 72%|███████▏  | 275/384 [02:39<01:19,  1.37it/s] 72%|███████▏  | 276/384 [02:39<01:13,  1.48it/s] 72%|███████▏  | 277/384 [02:40<01:08,  1.55it/s] 72%|███████▏  | 278/384 [02:41<01:05,  1.61it/s] 73%|███████▎  | 279/384 [02:41<01:03,  1.66it/s] 73%|███████▎  | 280/384 [02:42<01:01,  1.70it/s] 73%|███████▎  | 281/384 [02:42<01:00,  1.70it/s] 73%|███████▎  | 282/384 [02:43<01:00,  1.70it/s] 74%|███████▎  | 283/384 [02:43<00:58,  1.72it/s] 74%|███████▍  | 284/384 [02:44<00:57,  1.72it/s] 74%|███████▍  | 285/384 [02:45<00:59,  1.66it/s] 74%|███████▍  | 286/384 [02:45<00:58,  1.67it/s] 75%|███████▍  | 287/384 [02:46<00:57,  1.70it/s] 75%|███████▌  | 288/384 [02:46<00:55,  1.72it/s] 75%|███████▌  | 289/384 [02:47<00:55,  1.72it/s] 76%|███████▌  | 290/384 [02:48<00:54,  1.73it/s] 76%|███████▌  | 291/384 [02:48<00:53,  1.73it/s] 76%|███████▌  | 292/384 [02:49<00:52,  1.75it/s] 76%|███████▋  | 293/384 [02:49<00:52,  1.75it/s] 77%|███████▋  | 294/384 [02:50<00:51,  1.76it/s] 77%|███████▋  | 295/384 [02:50<00:50,  1.77it/s] 77%|███████▋  | 296/384 [02:51<00:50,  1.75it/s] 77%|███████▋  | 297/384 [02:52<00:49,  1.75it/s] 78%|███████▊  | 298/384 [02:52<00:49,  1.74it/s] 78%|███████▊  | 299/384 [02:53<00:49,  1.73it/s] 78%|███████▊  | 300/384 [02:53<00:49,  1.71it/s] 78%|███████▊  | 301/384 [02:54<00:48,  1.72it/s] 79%|███████▊  | 302/384 [02:54<00:47,  1.71it/s] 79%|███████▉  | 303/384 [02:55<00:47,  1.70it/s] 79%|███████▉  | 304/384 [02:56<00:46,  1.72it/s] 79%|███████▉  | 305/384 [02:56<00:46,  1.72it/s] 80%|███████▉  | 306/384 [02:57<00:45,  1.71it/s] 80%|███████▉  | 307/384 [02:57<00:45,  1.71it/s] 80%|████████  | 308/384 [02:58<00:45,  1.67it/s] 80%|████████  | 309/384 [02:59<00:46,  1.63it/s] 81%|████████  | 310/384 [02:59<00:45,  1.63it/s] 81%|████████  | 311/384 [03:00<00:44,  1.65it/s] 81%|████████▏ | 312/384 [03:00<00:43,  1.66it/s] 82%|████████▏ | 313/384 [03:01<00:43,  1.63it/s] 82%|████████▏ | 314/384 [03:02<00:42,  1.65it/s] 82%|████████▏ | 315/384 [03:02<00:42,  1.64it/s] 82%|████████▏ | 316/384 [03:03<00:40,  1.66it/s] 83%|████████▎ | 317/384 [03:03<00:40,  1.66it/s] 83%|████████▎ | 318/384 [03:04<00:39,  1.66it/s] 83%|████████▎ | 319/384 [03:05<00:39,  1.66it/s] 83%|████████▎ | 320/384 [03:05<00:38,  1.66it/s] 84%|████████▎ | 321/384 [03:06<00:37,  1.67it/s] 84%|████████▍ | 322/384 [03:06<00:37,  1.65it/s] 84%|████████▍ | 323/384 [03:07<00:36,  1.66it/s] 84%|████████▍ | 324/384 [03:08<00:36,  1.65it/s] 85%|████████▍ | 325/384 [03:08<00:35,  1.66it/s] 85%|████████▍ | 326/384 [03:09<00:35,  1.65it/s] 85%|████████▌ | 327/384 [03:10<00:35,  1.62it/s] 85%|████████▌ | 328/384 [03:10<00:34,  1.63it/s] 86%|████████▌ | 329/384 [03:11<00:33,  1.63it/s] 86%|████████▌ | 330/384 [03:11<00:33,  1.62it/s] 86%|████████▌ | 331/384 [03:12<00:32,  1.62it/s] 86%|████████▋ | 332/384 [03:13<00:31,  1.63it/s] 87%|████████▋ | 333/384 [03:13<00:31,  1.63it/s] 87%|████████▋ | 334/384 [03:14<00:31,  1.61it/s] 87%|████████▋ | 335/384 [03:14<00:30,  1.62it/s] 88%|████████▊ | 336/384 [03:15<00:29,  1.62it/s] 88%|████████▊ | 337/384 [03:16<00:29,  1.62it/s] 88%|████████▊ | 338/384 [03:16<00:28,  1.61it/s] 88%|████████▊ | 339/384 [03:17<00:27,  1.61it/s] 89%|████████▊ | 340/384 [03:18<00:28,  1.57it/s] 89%|████████▉ | 341/384 [03:18<00:27,  1.59it/s] 89%|████████▉ | 342/384 [03:19<00:28,  1.48it/s] 89%|████████▉ | 343/384 [03:20<00:27,  1.51it/s] 90%|████████▉ | 344/384 [03:20<00:26,  1.53it/s] 90%|████████▉ | 345/384 [03:21<00:26,  1.46it/s] 90%|█████████ | 346/384 [03:22<00:25,  1.49it/s] 90%|█████████ | 347/384 [03:22<00:24,  1.50it/s] 91%|█████████ | 348/384 [03:23<00:24,  1.46it/s] 91%|█████████ | 349/384 [03:24<00:23,  1.49it/s] 91%|█████████ | 350/384 [03:24<00:22,  1.49it/s] 91%|█████████▏| 351/384 [03:25<00:22,  1.48it/s] 92%|█████████▏| 352/384 [03:26<00:21,  1.49it/s] 92%|█████████▏| 353/384 [03:26<00:20,  1.53it/s] 92%|█████████▏| 354/384 [03:27<00:19,  1.54it/s] 92%|█████████▏| 355/384 [03:28<00:18,  1.54it/s] 93%|█████████▎| 356/384 [03:28<00:18,  1.54it/s] 93%|█████████▎| 357/384 [03:29<00:17,  1.55it/s] 93%|█████████▎| 358/384 [03:30<00:17,  1.45it/s] 93%|█████████▎| 359/384 [03:30<00:16,  1.48it/s] 94%|█████████▍| 360/384 [03:31<00:15,  1.51it/s] 94%|█████████▍| 361/384 [03:32<00:16,  1.42it/s] 94%|█████████▍| 362/384 [03:32<00:15,  1.45it/s] 95%|█████████▍| 363/384 [03:36<00:32,  1.55s/it] 95%|█████████▍| 364/384 [03:37<00:25,  1.28s/it] 95%|█████████▌| 365/384 [03:37<00:20,  1.10s/it] 95%|█████████▌| 366/384 [03:38<00:17,  1.03it/s] 96%|█████████▌| 367/384 [03:39<00:15,  1.09it/s] 96%|█████████▌| 368/384 [03:39<00:13,  1.19it/s] 96%|█████████▌| 369/384 [03:40<00:12,  1.21it/s] 96%|█████████▋| 370/384 [03:41<00:10,  1.28it/s] 97%|█████████▋| 371/384 [03:42<00:11,  1.12it/s] 97%|█████████▋| 372/384 [03:43<00:10,  1.18it/s] 97%|█████████▋| 373/384 [03:44<00:09,  1.19it/s] 97%|█████████▋| 374/384 [03:44<00:07,  1.26it/s] 98%|█████████▊| 375/384 [03:45<00:07,  1.24it/s] 98%|█████████▊| 376/384 [03:46<00:06,  1.31it/s] 98%|█████████▊| 377/384 [03:47<00:05,  1.25it/s] 98%|█████████▊| 378/384 [03:47<00:04,  1.28it/s] 99%|█████████▊| 379/384 [03:48<00:04,  1.22it/s] 99%|█████████▉| 380/384 [03:49<00:03,  1.17it/s] 99%|█████████▉| 381/384 [03:50<00:02,  1.06it/s] 99%|█████████▉| 382/384 [03:52<00:02,  1.03s/it]100%|█████████▉| 383/384 [03:53<00:01,  1.16s/it]100%|██████████| 384/384 [03:54<00:00,  1.06it/s]100%|██████████| 384/384 [03:54<00:00,  1.64it/s]Moving model to CUDA

all_samples: 12262
list_of_results: 12262
global MRR: 0.25335164068175126
global Precision at 10: 0.399363888435818
global Precision at 1: 0.17615397161963792


P@1 : 0.17615397161963792

2022-08-18 17:33:29 | INFO | fairseq.file_utils | loading archive file pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128
2022-08-18 17:33:30 | INFO | fairseq.tasks.masked_lm | dictionary: 51199 types
2022-08-18 17:33:33 | INFO | fairseq.models.roberta.model | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 10, 'log_format': 'json', 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': True, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 4, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 128, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://learnfair1212:17242', 'distributed_port': 17242, 'device_id': 0, 'distributed_no_spawn': True, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': True, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': True, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': None, 'batch_size': 32, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': True, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 500, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': 32, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 500000, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0006], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': '/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 1000, 'keep_interval_updates': 1, 'keep_interval_updates_pattern': 50000, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 128}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='roberta_base', activation_dropout=0.0, activation_fn='gelu', adam_betas='(0.9, 0.98)', adam_eps=1e-06, adaptive_input=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='roberta_base', attention_dropout=0.1, azureml_logging=False, batch_size=32, batch_size_valid=32, best_checkpoint_metric='loss', bf16=False, bpe='gpt2', broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=True, continue_once=None, cpu=False, cpu_offload=False, criterion='masked_lm', curriculum=0, data='/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=17242, distributed_rank=0, distributed_world_size=128, dropout=0.1, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=12, encoder_embed_dim=768, encoder_ffn_embed_dim=3072, encoder_layerdrop=0, encoder_layers=12, encoder_layers_to_keep=None, encoder_learned_pos=True, encoder_normalize_before=False, end_learning_rate=0.0, eos=2, fast_stat_sync=True, ffn_blocks_to_remove=-1, ffn_reg_scale_factor=0.0, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, force_anneal=None, fp16=True, fp16_adam_stats=True, fp16_init_scale=4, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freq_weighted_replacement=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_unused_valid_subsets=False, include_target_tokens=False, keep_best_checkpoints=-1, keep_interval_updates=1, keep_interval_updates_pattern=50000, keep_last_epochs=-1, layernorm_embedding=True, leave_unmasked_prob=0.1, load_checkpoint_heads=True, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format='json', log_interval=10, lr=[0.0006], lr_scheduler='polynomial_decay', mask_impl='fix', mask_multiple_length=1, mask_prob=0.2, mask_prob_range='none', mask_stdev=0.0, mask_whole_words=False, max_epoch=0, max_positions=512, max_source_positions=512, max_tokens=None, max_tokens_valid=None, max_update=500000, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=True, mha_heads_to_keep=-1, mha_reg_scale_factor=0.0, min_loss_scale=0.0001, min_params_to_wrap=100000000, model_parallel_size=1, no_epoch_checkpoints=True, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=True, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', pooler_activation_fn='tanh', pooler_dropout=0.0, power=1.0, profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, random_token_prob=0.1, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', sample_break_mode='complete', save_dir='/checkpoint/zeyuliu/en_dense_lm/masking_strategies/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', save_interval=1, save_interval_updates=1000, scoring='bleu', seed=1, sentence_avg=False, seqlen_masking_boundary='none', seqlen_masking_flag=False, seqlen_masking_granularity='none', shard_id=0, shorten_data_split_list='', shorten_method='none', simul_type=None, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, spectral_norm_classification_head=False, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='masked_lm', tensorboard_logdir='/checkpoint/zeyuliu/tensorboard_logs/2022-07-22/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', threshold_loss_scale=None, tokenizer=None, tokens_per_sample=512, total_num_update='500000', tpu=False, train_subset='train', unk=3, untie_weights_roberta=False, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=500, wandb_project=None, warmup_updates=24000, weight_decay=0.01, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'masked_lm', 'data': '/private/home/zeyuliu/masking_strategy/LAMA/pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128', 'sample_break_mode': 'complete', 'tokens_per_sample': 512, 'mask_prob': 0.2, 'mask_prob_range': 'none', 'mask_impl': 'fix', 'seqlen_masking_flag': False, 'seqlen_masking_granularity': 'none', 'seqlen_masking_boundary': 'none', 'leave_unmasked_prob': 0.1, 'random_token_prob': 0.1, 'freq_weighted_replacement': False, 'mask_whole_words': False, 'mask_multiple_length': 1, 'mask_stdev': 0.0, 'shorten_method': 'none', 'shorten_data_split_list': '', 'seed': 1, 'include_target_tokens': False}, 'criterion': {'_name': 'masked_lm', 'tpu': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-06, 'weight_decay': 0.01, 'use_old_adam': False, 'fp16_adam_stats': True, 'tpu': False, 'lr': [0.0006]}, 'lr_scheduler': {'_name': 'polynomial_decay', 'warmup_updates': 24000, 'force_anneal': None, 'end_learning_rate': 0.0, 'power': 1.0, 'total_num_update': 500000.0, 'lr': [0.0006]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': {'_name': 'gpt2', 'gpt2_encoder_json': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/encoder.json', 'gpt2_vocab_bpe': 'https://dl.fbaipublicfiles.com/fairseq/gpt2_bpe/vocab.bpe'}, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}
@@@ roberta_base_mp0.2 - mean P@1: 0.17615397161963792
4. SQuAD
roberta_base_mp0.2
{'relation': 'test'}
{'dataset_filename': 'data/Squad/test.jsonl', 'common_vocab_filename': 'pre-trained_language_models/common_vocab_cased.txt', 'template': '', 'bert_vocab_name': 'vocab.txt', 'batch_size': 32, 'logdir': 'output', 'full_logdir': 'output/results/roberta_base_mp0.2/test', 'lowercase': False, 'max_sentence_length': 512, 'threads': -1, 'interactive': False, 'use_negated_probes': False, 'lm': 'roberta', 'label': 'roberta_base_mp0.2', 'models_names': ['roberta'], 'roberta_model_name': 'checkpoint_52_500000.pt', 'roberta_vocab_name': 'dict.txt', 'roberta_model_dir': 'pre-trained_language_models/roberta.base.faststatsync.me_fp16.cmpltsents.mp0.2.roberta_base.tps512.adam.fp16adam.b2_0.98.eps1e-06.cl0.0.lr0.0006.wu24000.dr0.1.atdr0.1.wd0.01.ms32.uf2.mu500000.s1.ngpu128'}
Loading roberta model...
<lama.modules.roberta_connector.Roberta object at 0x7f26987e6810>
305
286
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:00<00:05,  1.50it/s] 22%|██▏       | 2/9 [00:01<00:04,  1.64it/s] 33%|███▎      | 3/9 [00:01<00:03,  1.67it/s] 44%|████▍     | 4/9 [00:02<00:02,  1.69it/s] 56%|█████▌    | 5/9 [00:02<00:02,  1.70it/s] 67%|██████▋   | 6/9 [00:03<00:01,  1.71it/s] 78%|███████▊  | 7/9 [00:04<00:01,  1.68it/s] 89%|████████▉ | 8/9 [00:04<00:00,  1.68it/s]100%|██████████| 9/9 [00:05<00:00,  1.70it/s]100%|██████████| 9/9 [00:05<00:00,  1.68it/s]Moving model to CUDA

all_samples: 286
list_of_results: 286
global MRR: 0.22226987170386317
global Precision at 10: 0.4195804195804196
global Precision at 1: 0.11538461538461539


P@1 : 0.11538461538461539

@@@ roberta_base_mp0.2 - mean P@1: 0.11538461538461539
